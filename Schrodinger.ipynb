{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJQAwuTyDLdPQCB+rKwvrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amiralizadeh1/PINNs/blob/master/Schrodinger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIpdavnYvFjz",
        "outputId": "05f87aa2-116e-4a39-b384-d2f4fbd7e83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 47 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (2.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.50.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==1.14.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K-hcn9cV1wO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svmq7Ukdvl2k",
        "outputId": "74092995-0b2e-4f95-ee8e-2f20d0155e55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pyDOE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXRyCs6_wWJh",
        "outputId": "0766fdcb-eed4-4674-a6b0-f71a92e6928e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.7.3)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=fecb9c5544d7bcbc7a8f9f1e88a51bf9317d6a2eb0f12518e18ef3bd52e2c001\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ce/8a/87b25c685bfeca1872d13b8dc101e087a9c6e3fb5ebb47022a\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@author: Maziar Raissi\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Utilities') #upload utilities folder to your google drive before running the lines\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "from plotting import newfig, savefig\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "\n",
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub):\n",
        "        \n",
        "        X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "        X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
        "        X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
        "        \n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "               \n",
        "        self.x0 = X0[:,0:1]\n",
        "        self.t0 = X0[:,1:2]\n",
        "\n",
        "        self.x_lb = X_lb[:,0:1]\n",
        "        self.t_lb = X_lb[:,1:2]\n",
        "\n",
        "        self.x_ub = X_ub[:,0:1]\n",
        "        self.t_ub = X_ub[:,1:2]\n",
        "        \n",
        "        self.x_f = X_f[:,0:1]\n",
        "        self.t_f = X_f[:,1:2]\n",
        "        \n",
        "        self.u0 = u0\n",
        "        self.v0 = v0\n",
        "        \n",
        "        # Initialize NNs\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # tf Placeholders        \n",
        "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
        "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "        \n",
        "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
        "        self.v0_tf = tf.placeholder(tf.float32, shape=[None, self.v0.shape[1]])\n",
        "        \n",
        "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
        "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
        "        \n",
        "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
        "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
        "        \n",
        "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
        "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "        # tf Graphs\n",
        "        self.u0_pred, self.v0_pred, _ , _ = self.net_uv(self.x0_tf, self.t0_tf)\n",
        "        self.u_lb_pred, self.v_lb_pred, self.u_x_lb_pred, self.v_x_lb_pred = self.net_uv(self.x_lb_tf, self.t_lb_tf)\n",
        "        self.u_ub_pred, self.v_ub_pred, self.u_x_ub_pred, self.v_x_ub_pred = self.net_uv(self.x_ub_tf, self.t_ub_tf)\n",
        "        self.f_u_pred, self.f_v_pred = self.net_f_uv(self.x_f_tf, self.t_f_tf)\n",
        "        \n",
        "        # Loss\n",
        "        self.loss = tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v0_tf - self.v0_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_lb_pred - self.v_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_x_lb_pred - self.v_x_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_v_pred))\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
        "                                                                method = 'L-BFGS-B', \n",
        "                                                                options = {'maxiter': 50000,\n",
        "                                                                           'maxfun': 50000,\n",
        "                                                                           'maxcor': 50,\n",
        "                                                                           'maxls': 50,\n",
        "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
        "    \n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "                \n",
        "        # tf session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "              \n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "    \n",
        "    def net_uv(self, x, t):\n",
        "        X = tf.concat([x,t],1)\n",
        "        \n",
        "        uv = self.neural_net(X, self.weights, self.biases)\n",
        "        u = uv[:,0:1]\n",
        "        v = uv[:,1:2]\n",
        "        \n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "\n",
        "        return u, v, u_x, v_x\n",
        "\n",
        "    def net_f_uv(self, x, t):\n",
        "        u, v, u_x, v_x = self.net_uv(x,t)\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        \n",
        "        v_t = tf.gradients(v, t)[0]\n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "        \n",
        "        f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v\n",
        "        f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u   \n",
        "        \n",
        "        return f_u, f_v\n",
        "    \n",
        "    def callback(self, loss):\n",
        "        print('Loss:', loss)\n",
        "        \n",
        "    def train(self, nIter):\n",
        "        \n",
        "        tf_dict = {self.x0_tf: self.x0, self.t0_tf: self.t0,\n",
        "                   self.u0_tf: self.u0, self.v0_tf: self.v0,\n",
        "                   self.x_lb_tf: self.x_lb, self.t_lb_tf: self.t_lb,\n",
        "                   self.x_ub_tf: self.x_ub, self.t_ub_tf: self.t_ub,\n",
        "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "                                                                                                                          \n",
        "        self.optimizer.minimize(self.sess, \n",
        "                                feed_dict = tf_dict,         \n",
        "                                fetches = [self.loss], \n",
        "                                loss_callback = self.callback)        \n",
        "                                    \n",
        "    \n",
        "    def predict(self, X_star):\n",
        "        \n",
        "        tf_dict = {self.x0_tf: X_star[:,0:1], self.t0_tf: X_star[:,1:2]}\n",
        "        \n",
        "        u_star = self.sess.run(self.u0_pred, tf_dict)  \n",
        "        v_star = self.sess.run(self.v0_pred, tf_dict)  \n",
        "        \n",
        "        \n",
        "        tf_dict = {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]}\n",
        "        \n",
        "        f_u_star = self.sess.run(self.f_u_pred, tf_dict)\n",
        "        f_v_star = self.sess.run(self.f_v_pred, tf_dict)\n",
        "               \n",
        "        return u_star, v_star, f_u_star, f_v_star\n",
        "    \n",
        "if __name__ == \"__main__\": \n",
        "     \n",
        "    noise = 0.0        \n",
        "    \n",
        "    # Doman bounds\n",
        "    lb = np.array([-5.0, 0.0])\n",
        "    ub = np.array([5.0, np.pi/2])\n",
        "\n",
        "    N0 = 50\n",
        "    N_b = 50\n",
        "    N_f = 20000\n",
        "    layers = [2, 100, 100, 100, 100, 2]\n",
        "        \n",
        "    data = scipy.io.loadmat('..//content/drive/MyDrive/Data/NLS.mat')\n",
        "    \n",
        "    t = data['tt'].flatten()[:,None]\n",
        "    x = data['x'].flatten()[:,None]\n",
        "    Exact = data['uu']\n",
        "    Exact_u = np.real(Exact)\n",
        "    Exact_v = np.imag(Exact)\n",
        "    Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
        "    \n",
        "    X, T = np.meshgrid(x,t)\n",
        "    \n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "    u_star = Exact_u.T.flatten()[:,None]\n",
        "    v_star = Exact_v.T.flatten()[:,None]\n",
        "    h_star = Exact_h.T.flatten()[:,None]\n",
        "    \n",
        "    ###########################\n",
        "    \n",
        "    idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "    x0 = x[idx_x,:]\n",
        "    u0 = Exact_u[idx_x,0:1]\n",
        "    v0 = Exact_v[idx_x,0:1]\n",
        "    \n",
        "    idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "    tb = t[idx_t,:]\n",
        "    \n",
        "    X_f = lb + (ub-lb)*lhs(2, N_f)\n",
        "            \n",
        "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub)\n",
        "             \n",
        "    start_time = time.time()                \n",
        "    #model.train(50000)\n",
        "    model.train(500)\n",
        "    elapsed = time.time() - start_time                \n",
        "    print('Training time: %.4f' % (elapsed))\n",
        "    \n",
        "        \n",
        "    u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
        "    h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
        "            \n",
        "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "    error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
        "    error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    print('Error v: %e' % (error_v))\n",
        "    print('Error h: %e' % (error_h))\n",
        "\n",
        "    \n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
        "    H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
        "    FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')     \n",
        "    \n",
        "\n",
        "    \n",
        "    ######################################################################\n",
        "    ############################# Plotting ###############################\n",
        "    ######################################################################    \n",
        "    \n",
        "    X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "    X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
        "    X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
        "    X_u_train = np.vstack([X0, X_lb, X_ub])\n",
        "\n",
        "    fig, ax = newfig(1.0, 0.9)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    ####### Row 0: h(t,x) ##################    \n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "    h = ax.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu', \n",
        "                  extent=[lb[1], ub[1], lb[0], ub[0]], \n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "    \n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (X_u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "    \n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "    ax.plot(t[100]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "    ax.plot(t[125]*np.ones((2,1)), line, 'k--', linewidth = 1)    \n",
        "    \n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    leg = ax.legend(frameon=False, loc = 'best')\n",
        "#    plt.setp(leg.get_texts(), color='w')\n",
        "    ax.set_title('$|h(t,x)|$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 1: h(t,x) slices ##################    \n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$|h(t,x)|$')    \n",
        "    ax.set_title('$t = %.2f$' % (t[75]), fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-5.1,5.1])\n",
        "    ax.set_ylim([-0.1,5.1])\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,Exact_h[:,100], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,H_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$|h(t,x)|$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-5.1,5.1])\n",
        "    ax.set_ylim([-0.1,5.1])\n",
        "    ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$|h(t,x)|$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-5.1,5.1])\n",
        "    ax.set_ylim([-0.1,5.1])    \n",
        "    ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)\n",
        "    \n",
        "    savefig('./figures/NLS')  \n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7BryzQKXvtAF",
        "outputId": "c26f0d80-3280-43c9-b52c-a3d908b39c57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "It: 0, Loss: 7.523e-01, Time: 3.64\n",
            "It: 10, Loss: 5.369e-01, Time: 11.31\n",
            "It: 20, Loss: 4.611e-01, Time: 11.15\n",
            "It: 30, Loss: 4.520e-01, Time: 11.19\n",
            "It: 40, Loss: 4.435e-01, Time: 17.53\n",
            "It: 50, Loss: 4.402e-01, Time: 12.06\n",
            "It: 60, Loss: 4.316e-01, Time: 11.11\n",
            "It: 70, Loss: 4.085e-01, Time: 12.85\n",
            "It: 80, Loss: 3.558e-01, Time: 11.29\n",
            "It: 90, Loss: 2.597e-01, Time: 12.22\n",
            "It: 100, Loss: 1.972e-01, Time: 13.82\n",
            "It: 110, Loss: 1.673e-01, Time: 11.05\n",
            "It: 120, Loss: 1.574e-01, Time: 11.10\n",
            "It: 130, Loss: 1.531e-01, Time: 11.44\n",
            "It: 140, Loss: 1.393e-01, Time: 11.75\n",
            "It: 150, Loss: 1.280e-01, Time: 11.15\n",
            "It: 160, Loss: 1.179e-01, Time: 11.34\n",
            "It: 170, Loss: 1.082e-01, Time: 11.22\n",
            "It: 180, Loss: 9.893e-02, Time: 11.12\n",
            "It: 190, Loss: 9.168e-02, Time: 11.45\n",
            "It: 200, Loss: 9.294e-02, Time: 11.53\n",
            "It: 210, Loss: 8.848e-02, Time: 11.17\n",
            "It: 220, Loss: 8.247e-02, Time: 11.13\n",
            "It: 230, Loss: 7.771e-02, Time: 11.11\n",
            "It: 240, Loss: 7.515e-02, Time: 10.98\n",
            "It: 250, Loss: 7.312e-02, Time: 11.05\n",
            "It: 260, Loss: 7.359e-02, Time: 11.03\n",
            "It: 270, Loss: 6.990e-02, Time: 10.98\n",
            "It: 280, Loss: 6.844e-02, Time: 11.99\n",
            "It: 290, Loss: 6.729e-02, Time: 11.05\n",
            "It: 300, Loss: 6.613e-02, Time: 10.98\n",
            "It: 310, Loss: 6.508e-02, Time: 11.07\n",
            "It: 320, Loss: 6.413e-02, Time: 11.02\n",
            "It: 330, Loss: 6.314e-02, Time: 11.17\n",
            "It: 340, Loss: 6.224e-02, Time: 11.11\n",
            "It: 350, Loss: 6.138e-02, Time: 16.65\n",
            "It: 360, Loss: 6.057e-02, Time: 12.00\n",
            "It: 370, Loss: 5.979e-02, Time: 11.05\n",
            "It: 380, Loss: 5.906e-02, Time: 11.13\n",
            "It: 390, Loss: 5.839e-02, Time: 11.03\n",
            "It: 400, Loss: 8.432e-02, Time: 11.01\n",
            "It: 410, Loss: 5.756e-02, Time: 11.07\n",
            "It: 420, Loss: 5.690e-02, Time: 11.03\n",
            "It: 430, Loss: 5.615e-02, Time: 11.58\n",
            "It: 440, Loss: 5.555e-02, Time: 11.06\n",
            "It: 450, Loss: 5.499e-02, Time: 10.97\n",
            "It: 460, Loss: 5.438e-02, Time: 11.01\n",
            "It: 470, Loss: 5.373e-02, Time: 11.03\n",
            "It: 480, Loss: 5.311e-02, Time: 10.95\n",
            "It: 490, Loss: 5.246e-02, Time: 11.04\n",
            "Loss: 0.05301504\n",
            "Loss: 65963.29\n",
            "Loss: 30.480446\n",
            "Loss: 0.4420751\n",
            "Loss: 0.051956367\n",
            "Loss: 0.051900443\n",
            "Loss: 0.051855616\n",
            "Loss: 0.05185595\n",
            "Loss: 0.05185258\n",
            "Loss: 0.051851463\n",
            "Loss: 0.051847763\n",
            "Loss: 0.0518362\n",
            "Loss: 0.051817957\n",
            "Loss: 0.051762305\n",
            "Loss: 0.05168175\n",
            "Loss: 0.051566746\n",
            "Loss: 0.05112128\n",
            "Loss: 0.050721988\n",
            "Loss: 0.049648248\n",
            "Loss: 0.048796207\n",
            "Loss: 0.048034202\n",
            "Loss: 0.04759685\n",
            "Loss: 0.04734494\n",
            "Loss: 0.0470007\n",
            "Loss: 0.046556845\n",
            "Loss: 0.04609953\n",
            "Loss: 0.045828853\n",
            "Loss: 0.045605\n",
            "Loss: 0.04538194\n",
            "Loss: 0.04488889\n",
            "Loss: 0.044545285\n",
            "Loss: 0.044341005\n",
            "Loss: 0.043962423\n",
            "Loss: 0.04329419\n",
            "Loss: 0.042821024\n",
            "Loss: 0.042506766\n",
            "Loss: 0.042340253\n",
            "Loss: 0.0421229\n",
            "Loss: 0.04181961\n",
            "Loss: 0.041626267\n",
            "Loss: 0.04139625\n",
            "Loss: 0.04112826\n",
            "Loss: 0.040923014\n",
            "Loss: 0.040701814\n",
            "Loss: 0.04039284\n",
            "Loss: 0.040177733\n",
            "Loss: 0.039940692\n",
            "Loss: 0.039831925\n",
            "Loss: 0.0397048\n",
            "Loss: 0.03943192\n",
            "Loss: 0.039055377\n",
            "Loss: 0.038830437\n",
            "Loss: 0.03868352\n",
            "Loss: 0.03860189\n",
            "Loss: 0.03846878\n",
            "Loss: 0.038310684\n",
            "Loss: 0.03807826\n",
            "Loss: 0.037579637\n",
            "Loss: 0.037415367\n",
            "Loss: 0.036944926\n",
            "Loss: 0.03671676\n",
            "Loss: 0.03660903\n",
            "Loss: 0.03655224\n",
            "Loss: 0.036517058\n",
            "Loss: 0.036483187\n",
            "Loss: 0.036416106\n",
            "Loss: 0.036244906\n",
            "Loss: 0.036041077\n",
            "Loss: 0.035845757\n",
            "Loss: 0.03572799\n",
            "Loss: 0.035670053\n",
            "Loss: 0.035593487\n",
            "Loss: 0.035487354\n",
            "Loss: 0.035383444\n",
            "Loss: 0.035302058\n",
            "Loss: 0.03523819\n",
            "Loss: 0.035159685\n",
            "Loss: 0.035009146\n",
            "Loss: 0.03477997\n",
            "Loss: 0.035160117\n",
            "Loss: 0.034670696\n",
            "Loss: 0.034521658\n",
            "Loss: 0.03442712\n",
            "Loss: 0.03432644\n",
            "Loss: 0.034189716\n",
            "Loss: 0.0340349\n",
            "Loss: 0.033872884\n",
            "Loss: 0.033757403\n",
            "Loss: 0.033631768\n",
            "Loss: 0.03351612\n",
            "Loss: 0.03333137\n",
            "Loss: 0.03315954\n",
            "Loss: 0.03301548\n",
            "Loss: 0.032893665\n",
            "Loss: 0.03282091\n",
            "Loss: 0.032723103\n",
            "Loss: 0.032594364\n",
            "Loss: 0.032483924\n",
            "Loss: 0.032397386\n",
            "Loss: 0.03233377\n",
            "Loss: 0.032276012\n",
            "Loss: 0.0321817\n",
            "Loss: 0.0319798\n",
            "Loss: 0.031766232\n",
            "Loss: 0.031711947\n",
            "Loss: 0.0316264\n",
            "Loss: 0.031581707\n",
            "Loss: 0.031488914\n",
            "Loss: 0.03131563\n",
            "Loss: 0.031073824\n",
            "Loss: 0.03091887\n",
            "Loss: 0.030918181\n",
            "Loss: 0.030854464\n",
            "Loss: 0.030799728\n",
            "Loss: 0.030760525\n",
            "Loss: 0.030669983\n",
            "Loss: 0.030530432\n",
            "Loss: 0.030536331\n",
            "Loss: 0.030469168\n",
            "Loss: 0.030414157\n",
            "Loss: 0.03031745\n",
            "Loss: 0.030193299\n",
            "Loss: 0.030042013\n",
            "Loss: 0.029976837\n",
            "Loss: 0.029907264\n",
            "Loss: 0.029853359\n",
            "Loss: 0.029770397\n",
            "Loss: 0.029609561\n",
            "Loss: 0.029401299\n",
            "Loss: 0.02923442\n",
            "Loss: 0.029091416\n",
            "Loss: 0.028984848\n",
            "Loss: 0.028923722\n",
            "Loss: 0.028853387\n",
            "Loss: 0.028746145\n",
            "Loss: 0.028695513\n",
            "Loss: 0.028631225\n",
            "Loss: 0.028523125\n",
            "Loss: 0.028442943\n",
            "Loss: 0.02837253\n",
            "Loss: 0.028249318\n",
            "Loss: 0.028147653\n",
            "Loss: 0.028060466\n",
            "Loss: 0.027964406\n",
            "Loss: 0.027878806\n",
            "Loss: 0.027793614\n",
            "Loss: 0.027735205\n",
            "Loss: 0.0276879\n",
            "Loss: 0.027617533\n",
            "Loss: 0.027543569\n",
            "Loss: 0.027433554\n",
            "Loss: 0.027330842\n",
            "Loss: 0.027210852\n",
            "Loss: 0.027144974\n",
            "Loss: 0.027087124\n",
            "Loss: 0.027018068\n",
            "Loss: 0.026889864\n",
            "Loss: 0.026766006\n",
            "Loss: 0.026690766\n",
            "Loss: 0.026542846\n",
            "Loss: 0.026443308\n",
            "Loss: 0.026350766\n",
            "Loss: 0.026249198\n",
            "Loss: 0.026148148\n",
            "Loss: 0.02733446\n",
            "Loss: 0.02613334\n",
            "Loss: 0.026056396\n",
            "Loss: 0.025982657\n",
            "Loss: 0.025858043\n",
            "Loss: 0.02575418\n",
            "Loss: 0.025708074\n",
            "Loss: 0.02559068\n",
            "Loss: 0.025543606\n",
            "Loss: 0.025499508\n",
            "Loss: 0.025403198\n",
            "Loss: 0.02546128\n",
            "Loss: 0.025330838\n",
            "Loss: 0.02522571\n",
            "Loss: 0.02507402\n",
            "Loss: 0.025064643\n",
            "Loss: 0.02501084\n",
            "Loss: 0.024905803\n",
            "Loss: 0.024827156\n",
            "Loss: 0.024707917\n",
            "Loss: 0.024605133\n",
            "Loss: 0.02463724\n",
            "Loss: 0.024557093\n",
            "Loss: 0.024476752\n",
            "Loss: 0.024415854\n",
            "Loss: 0.024356928\n",
            "Loss: 0.024277883\n",
            "Loss: 0.02417201\n",
            "Loss: 0.024094414\n",
            "Loss: 0.024043337\n",
            "Loss: 0.023979494\n",
            "Loss: 0.023907451\n",
            "Loss: 0.02377525\n",
            "Loss: 0.023657845\n",
            "Loss: 0.023553604\n",
            "Loss: 0.02347132\n",
            "Loss: 0.023396939\n",
            "Loss: 0.023324003\n",
            "Loss: 0.023282193\n",
            "Loss: 0.023238786\n",
            "Loss: 0.023202378\n",
            "Loss: 0.023163274\n",
            "Loss: 0.023185864\n",
            "Loss: 0.023138326\n",
            "Loss: 0.023094995\n",
            "Loss: 0.022997277\n",
            "Loss: 0.022913834\n",
            "Loss: 0.02280904\n",
            "Loss: 0.022724258\n",
            "Loss: 0.022614801\n",
            "Loss: 0.02251988\n",
            "Loss: 0.022505142\n",
            "Loss: 0.022381278\n",
            "Loss: 0.022320673\n",
            "Loss: 0.022215804\n",
            "Loss: 0.022145528\n",
            "Loss: 0.022079328\n",
            "Loss: 0.022021918\n",
            "Loss: 0.02195564\n",
            "Loss: 0.021903403\n",
            "Loss: 0.021844625\n",
            "Loss: 0.021784844\n",
            "Loss: 0.021756576\n",
            "Loss: 0.021704424\n",
            "Loss: 0.02166194\n",
            "Loss: 0.021593822\n",
            "Loss: 0.021506699\n",
            "Loss: 0.021421118\n",
            "Loss: 0.021325294\n",
            "Loss: 0.02122851\n",
            "Loss: 0.0211709\n",
            "Loss: 0.021097302\n",
            "Loss: 0.021040695\n",
            "Loss: 0.02097708\n",
            "Loss: 0.020925345\n",
            "Loss: 0.020851994\n",
            "Loss: 0.02081801\n",
            "Loss: 0.020749182\n",
            "Loss: 0.020700438\n",
            "Loss: 0.020645304\n",
            "Loss: 0.020554472\n",
            "Loss: 0.020520287\n",
            "Loss: 0.020450199\n",
            "Loss: 0.020387745\n",
            "Loss: 0.020337373\n",
            "Loss: 0.02024227\n",
            "Loss: 0.020244028\n",
            "Loss: 0.020189047\n",
            "Loss: 0.020117518\n",
            "Loss: 0.020040998\n",
            "Loss: 0.019954689\n",
            "Loss: 0.019818377\n",
            "Loss: 0.019725481\n",
            "Loss: 0.019660925\n",
            "Loss: 0.019622441\n",
            "Loss: 0.019586638\n",
            "Loss: 0.01946348\n",
            "Loss: 0.019334607\n",
            "Loss: 0.019214228\n",
            "Loss: 0.019056682\n",
            "Loss: 0.019020824\n",
            "Loss: 0.018906752\n",
            "Loss: 0.018860403\n",
            "Loss: 0.018782537\n",
            "Loss: 0.018695282\n",
            "Loss: 0.01857785\n",
            "Loss: 0.018471383\n",
            "Loss: 0.018363006\n",
            "Loss: 0.01825789\n",
            "Loss: 0.018181652\n",
            "Loss: 0.018116783\n",
            "Loss: 0.01808204\n",
            "Loss: 0.01804591\n",
            "Loss: 0.018005714\n",
            "Loss: 0.017937252\n",
            "Loss: 0.01790313\n",
            "Loss: 0.017839864\n",
            "Loss: 0.017780371\n",
            "Loss: 0.017705943\n",
            "Loss: 0.01762368\n",
            "Loss: 0.017482946\n",
            "Loss: 0.017386816\n",
            "Loss: 0.017292641\n",
            "Loss: 0.017235365\n",
            "Loss: 0.017190823\n",
            "Loss: 0.017145291\n",
            "Loss: 0.017081771\n",
            "Loss: 0.01701633\n",
            "Loss: 0.017119028\n",
            "Loss: 0.016972845\n",
            "Loss: 0.016884033\n",
            "Loss: 0.016806923\n",
            "Loss: 0.016706388\n",
            "Loss: 0.016580299\n",
            "Loss: 0.016451895\n",
            "Loss: 0.016377272\n",
            "Loss: 0.016311057\n",
            "Loss: 0.016240053\n",
            "Loss: 0.016162246\n",
            "Loss: 0.016102472\n",
            "Loss: 0.016062345\n",
            "Loss: 0.016031925\n",
            "Loss: 0.015989618\n",
            "Loss: 0.015936423\n",
            "Loss: 0.01595274\n",
            "Loss: 0.015908383\n",
            "Loss: 0.015862163\n",
            "Loss: 0.01576829\n",
            "Loss: 0.01567491\n",
            "Loss: 0.015573857\n",
            "Loss: 0.0155235045\n",
            "Loss: 0.015474793\n",
            "Loss: 0.015429134\n",
            "Loss: 0.015354382\n",
            "Loss: 0.015274088\n",
            "Loss: 0.015219704\n",
            "Loss: 0.015174789\n",
            "Loss: 0.015115391\n",
            "Loss: 0.015046561\n",
            "Loss: 0.014996057\n",
            "Loss: 0.014952127\n",
            "Loss: 0.01489119\n",
            "Loss: 0.014800191\n",
            "Loss: 0.014676422\n",
            "Loss: 0.0145925805\n",
            "Loss: 0.014473994\n",
            "Loss: 0.014428133\n",
            "Loss: 0.014382326\n",
            "Loss: 0.014340442\n",
            "Loss: 0.014289026\n",
            "Loss: 0.014302248\n",
            "Loss: 0.01424898\n",
            "Loss: 0.014207315\n",
            "Loss: 0.014165845\n",
            "Loss: 0.014118535\n",
            "Loss: 0.014032194\n",
            "Loss: 0.014019236\n",
            "Loss: 0.013920721\n",
            "Loss: 0.013863069\n",
            "Loss: 0.013790609\n",
            "Loss: 0.013734443\n",
            "Loss: 0.013684376\n",
            "Loss: 0.013634138\n",
            "Loss: 0.013575222\n",
            "Loss: 0.013808504\n",
            "Loss: 0.01354969\n",
            "Loss: 0.0134847965\n",
            "Loss: 0.013421634\n",
            "Loss: 0.01337523\n",
            "Loss: 0.013306888\n",
            "Loss: 0.013224422\n",
            "Loss: 0.013158937\n",
            "Loss: 0.013103927\n",
            "Loss: 0.013052564\n",
            "Loss: 0.013010388\n",
            "Loss: 0.012958553\n",
            "Loss: 0.012900384\n",
            "Loss: 0.012852354\n",
            "Loss: 0.012787266\n",
            "Loss: 0.012729835\n",
            "Loss: 0.012680179\n",
            "Loss: 0.012622505\n",
            "Loss: 0.012566872\n",
            "Loss: 0.012517905\n",
            "Loss: 0.012600303\n",
            "Loss: 0.012489124\n",
            "Loss: 0.012456802\n",
            "Loss: 0.012426386\n",
            "Loss: 0.012387082\n",
            "Loss: 0.012337942\n",
            "Loss: 0.012300018\n",
            "Loss: 0.0122567285\n",
            "Loss: 0.012209009\n",
            "Loss: 0.012152769\n",
            "Loss: 0.012082021\n",
            "Loss: 0.012015693\n",
            "Loss: 0.011943873\n",
            "Loss: 0.0118936775\n",
            "Loss: 0.01183382\n",
            "Loss: 0.011793255\n",
            "Loss: 0.011761872\n",
            "Loss: 0.011732623\n",
            "Loss: 0.011682812\n",
            "Loss: 0.011634303\n",
            "Loss: 0.011572541\n",
            "Loss: 0.011510531\n",
            "Loss: 0.011493906\n",
            "Loss: 0.011421354\n",
            "Loss: 0.011378237\n",
            "Loss: 0.011326116\n",
            "Loss: 0.011267232\n",
            "Loss: 0.011210979\n",
            "Loss: 0.011208621\n",
            "Loss: 0.011164408\n",
            "Loss: 0.011123562\n",
            "Loss: 0.011074768\n",
            "Loss: 0.0110338805\n",
            "Loss: 0.010998651\n",
            "Loss: 0.010973993\n",
            "Loss: 0.010950977\n",
            "Loss: 0.010901346\n",
            "Loss: 0.010870837\n",
            "Loss: 0.0108220335\n",
            "Loss: 0.01077912\n",
            "Loss: 0.0107318945\n",
            "Loss: 0.0106909545\n",
            "Loss: 0.01065233\n",
            "Loss: 0.010598952\n",
            "Loss: 0.010549688\n",
            "Loss: 0.010512707\n",
            "Loss: 0.010475627\n",
            "Loss: 0.010393268\n",
            "Loss: 0.010313033\n",
            "Loss: 0.01021974\n",
            "Loss: 0.010206565\n",
            "Loss: 0.0101687955\n",
            "Loss: 0.010117838\n",
            "Loss: 0.01009457\n",
            "Loss: 0.010066066\n",
            "Loss: 0.0100133205\n",
            "Loss: 0.0099597275\n",
            "Loss: 0.009937628\n",
            "Loss: 0.009847973\n",
            "Loss: 0.009798479\n",
            "Loss: 0.009752801\n",
            "Loss: 0.009685429\n",
            "Loss: 0.009609699\n",
            "Loss: 0.009549383\n",
            "Loss: 0.009490084\n",
            "Loss: 0.009442511\n",
            "Loss: 0.009390288\n",
            "Loss: 0.009312459\n",
            "Loss: 0.0092618335\n",
            "Loss: 0.00920525\n",
            "Loss: 0.00915034\n",
            "Loss: 0.009093933\n",
            "Loss: 0.009044394\n",
            "Loss: 0.008992692\n",
            "Loss: 0.008949632\n",
            "Loss: 0.008899944\n",
            "Loss: 0.00883488\n",
            "Loss: 0.00880084\n",
            "Loss: 0.008778402\n",
            "Loss: 0.008749364\n",
            "Loss: 0.00873173\n",
            "Loss: 0.008693603\n",
            "Loss: 0.008658353\n",
            "Loss: 0.008620279\n",
            "Loss: 0.008592266\n",
            "Loss: 0.00856892\n",
            "Loss: 0.00852971\n",
            "Loss: 0.008497444\n",
            "Loss: 0.008459196\n",
            "Loss: 0.008422533\n",
            "Loss: 0.008384338\n",
            "Loss: 0.008342061\n",
            "Loss: 0.008321743\n",
            "Loss: 0.008272599\n",
            "Loss: 0.008235972\n",
            "Loss: 0.008164339\n",
            "Loss: 0.00814073\n",
            "Loss: 0.008098378\n",
            "Loss: 0.008081019\n",
            "Loss: 0.008061441\n",
            "Loss: 0.008022941\n",
            "Loss: 0.007982835\n",
            "Loss: 0.007957588\n",
            "Loss: 0.007935952\n",
            "Loss: 0.007916553\n",
            "Loss: 0.007898968\n",
            "Loss: 0.007875485\n",
            "Loss: 0.007850255\n",
            "Loss: 0.007821325\n",
            "Loss: 0.007793624\n",
            "Loss: 0.0077666584\n",
            "Loss: 0.0077363173\n",
            "Loss: 0.0076992754\n",
            "Loss: 0.007669328\n",
            "Loss: 0.0076357448\n",
            "Loss: 0.0076059517\n",
            "Loss: 0.0075628534\n",
            "Loss: 0.007530897\n",
            "Loss: 0.00750471\n",
            "Loss: 0.0074706003\n",
            "Loss: 0.007441096\n",
            "Loss: 0.007404212\n",
            "Loss: 0.007383649\n",
            "Loss: 0.0073426086\n",
            "Loss: 0.0073227184\n",
            "Loss: 0.007299072\n",
            "Loss: 0.007279872\n",
            "Loss: 0.0072545754\n",
            "Loss: 0.0072287265\n",
            "Loss: 0.007205312\n",
            "Loss: 0.007171466\n",
            "Loss: 0.007115622\n",
            "Loss: 0.007058936\n",
            "Loss: 0.0070261676\n",
            "Loss: 0.0069998996\n",
            "Loss: 0.00697635\n",
            "Loss: 0.0069512203\n",
            "Loss: 0.0069056423\n",
            "Loss: 0.0068918336\n",
            "Loss: 0.0068739643\n",
            "Loss: 0.006846588\n",
            "Loss: 0.006822995\n",
            "Loss: 0.0067911837\n",
            "Loss: 0.006755185\n",
            "Loss: 0.006729705\n",
            "Loss: 0.0066976994\n",
            "Loss: 0.0066813664\n",
            "Loss: 0.006668221\n",
            "Loss: 0.006651419\n",
            "Loss: 0.006633724\n",
            "Loss: 0.0066081863\n",
            "Loss: 0.0065741558\n",
            "Loss: 0.0065537426\n",
            "Loss: 0.0065342896\n",
            "Loss: 0.0064960606\n",
            "Loss: 0.006467177\n",
            "Loss: 0.0064318283\n",
            "Loss: 0.0063976645\n",
            "Loss: 0.0063803336\n",
            "Loss: 0.0063554165\n",
            "Loss: 0.0063325833\n",
            "Loss: 0.0063057384\n",
            "Loss: 0.006290458\n",
            "Loss: 0.006260515\n",
            "Loss: 0.006237947\n",
            "Loss: 0.0062023085\n",
            "Loss: 0.0061761215\n",
            "Loss: 0.006147396\n",
            "Loss: 0.006127373\n",
            "Loss: 0.00613202\n",
            "Loss: 0.0061133197\n",
            "Loss: 0.0060912534\n",
            "Loss: 0.0060718157\n",
            "Loss: 0.0060541704\n",
            "Loss: 0.0060321884\n",
            "Loss: 0.005995586\n",
            "Loss: 0.005993284\n",
            "Loss: 0.005974349\n",
            "Loss: 0.005953914\n",
            "Loss: 0.005939551\n",
            "Loss: 0.0059247497\n",
            "Loss: 0.0059028524\n",
            "Loss: 0.005975555\n",
            "Loss: 0.0058923922\n",
            "Loss: 0.0058784327\n",
            "Loss: 0.0058576344\n",
            "Loss: 0.0058470857\n",
            "Loss: 0.00581488\n",
            "Loss: 0.0057732062\n",
            "Loss: 0.00577699\n",
            "Loss: 0.005761501\n",
            "Loss: 0.0057491898\n",
            "Loss: 0.0057325102\n",
            "Loss: 0.005707039\n",
            "Loss: 0.0056908703\n",
            "Loss: 0.005658899\n",
            "Loss: 0.005640218\n",
            "Loss: 0.005624552\n",
            "Loss: 0.005601216\n",
            "Loss: 0.005573016\n",
            "Loss: 0.0055437256\n",
            "Loss: 0.005511591\n",
            "Loss: 0.0054675783\n",
            "Loss: 0.0054356623\n",
            "Loss: 0.0054101953\n",
            "Loss: 0.0053973775\n",
            "Loss: 0.005381479\n",
            "Loss: 0.005359911\n",
            "Loss: 0.0053416844\n",
            "Loss: 0.005318433\n",
            "Loss: 0.005306091\n",
            "Loss: 0.005286387\n",
            "Loss: 0.005267523\n",
            "Loss: 0.0052469405\n",
            "Loss: 0.005230154\n",
            "Loss: 0.005205636\n",
            "Loss: 0.005185877\n",
            "Loss: 0.0051474525\n",
            "Loss: 0.005135674\n",
            "Loss: 0.0051062866\n",
            "Loss: 0.0050944416\n",
            "Loss: 0.0050746873\n",
            "Loss: 0.0050512264\n",
            "Loss: 0.0050188554\n",
            "Loss: 0.0049943547\n",
            "Loss: 0.0049779983\n",
            "Loss: 0.004963335\n",
            "Loss: 0.0049531437\n",
            "Loss: 0.004937083\n",
            "Loss: 0.0049146907\n",
            "Loss: 0.004903555\n",
            "Loss: 0.0048833042\n",
            "Loss: 0.004923412\n",
            "Loss: 0.004867648\n",
            "Loss: 0.0048433384\n",
            "Loss: 0.0048071253\n",
            "Loss: 0.00478357\n",
            "Loss: 0.004792626\n",
            "Loss: 0.0047672605\n",
            "Loss: 0.004740949\n",
            "Loss: 0.004716998\n",
            "Loss: 0.004691258\n",
            "Loss: 0.004673488\n",
            "Loss: 0.0046587624\n",
            "Loss: 0.004647555\n",
            "Loss: 0.004634779\n",
            "Loss: 0.0046215653\n",
            "Loss: 0.004612251\n",
            "Loss: 0.0045998557\n",
            "Loss: 0.004588578\n",
            "Loss: 0.0045648906\n",
            "Loss: 0.0045360443\n",
            "Loss: 0.004514739\n",
            "Loss: 0.0044952296\n",
            "Loss: 0.004478354\n",
            "Loss: 0.0044654314\n",
            "Loss: 0.004443184\n",
            "Loss: 0.0044268174\n",
            "Loss: 0.004420126\n",
            "Loss: 0.0044010505\n",
            "Loss: 0.0043847654\n",
            "Loss: 0.0043648374\n",
            "Loss: 0.004346664\n",
            "Loss: 0.004489064\n",
            "Loss: 0.004341592\n",
            "Loss: 0.00432666\n",
            "Loss: 0.004315829\n",
            "Loss: 0.004306386\n",
            "Loss: 0.0042971848\n",
            "Loss: 0.0042848643\n",
            "Loss: 0.0042765415\n",
            "Loss: 0.0042663603\n",
            "Loss: 0.004256841\n",
            "Loss: 0.004249658\n",
            "Loss: 0.0042389324\n",
            "Loss: 0.0042208666\n",
            "Loss: 0.004206161\n",
            "Loss: 0.004194494\n",
            "Loss: 0.0041822623\n",
            "Loss: 0.0041659353\n",
            "Loss: 0.0042387983\n",
            "Loss: 0.004158856\n",
            "Loss: 0.004144273\n",
            "Loss: 0.0041269003\n",
            "Loss: 0.00410602\n",
            "Loss: 0.0040845005\n",
            "Loss: 0.004063731\n",
            "Loss: 0.00404891\n",
            "Loss: 0.004030867\n",
            "Loss: 0.0040144455\n",
            "Loss: 0.003999197\n",
            "Loss: 0.0039734943\n",
            "Loss: 0.003961073\n",
            "Loss: 0.003950187\n",
            "Loss: 0.0039379206\n",
            "Loss: 0.0039091576\n",
            "Loss: 0.0038929982\n",
            "Loss: 0.003881765\n",
            "Loss: 0.0038742102\n",
            "Loss: 0.0038634548\n",
            "Loss: 0.0038497895\n",
            "Loss: 0.003839177\n",
            "Loss: 0.003831904\n",
            "Loss: 0.0038249192\n",
            "Loss: 0.0038054388\n",
            "Loss: 0.0038189958\n",
            "Loss: 0.0037993253\n",
            "Loss: 0.0037882146\n",
            "Loss: 0.0037816945\n",
            "Loss: 0.0037741058\n",
            "Loss: 0.0037617183\n",
            "Loss: 0.0037408322\n",
            "Loss: 0.003721539\n",
            "Loss: 0.0037085512\n",
            "Loss: 0.003696728\n",
            "Loss: 0.0036968687\n",
            "Loss: 0.0036905836\n",
            "Loss: 0.0036825829\n",
            "Loss: 0.0036677127\n",
            "Loss: 0.0036608218\n",
            "Loss: 0.0036463118\n",
            "Loss: 0.0036348775\n",
            "Loss: 0.0036366382\n",
            "Loss: 0.0036251978\n",
            "Loss: 0.0036173007\n",
            "Loss: 0.0036134091\n",
            "Loss: 0.0036082806\n",
            "Loss: 0.003592528\n",
            "Loss: 0.0035846033\n",
            "Loss: 0.0035756687\n",
            "Loss: 0.0035691338\n",
            "Loss: 0.0035638257\n",
            "Loss: 0.0035576895\n",
            "Loss: 0.0035483558\n",
            "Loss: 0.0035261177\n",
            "Loss: 0.00351459\n",
            "Loss: 0.0035009421\n",
            "Loss: 0.0034950245\n",
            "Loss: 0.0034844556\n",
            "Loss: 0.0034744286\n",
            "Loss: 0.0034636469\n",
            "Loss: 0.003451528\n",
            "Loss: 0.0034426888\n",
            "Loss: 0.003426045\n",
            "Loss: 0.0034153548\n",
            "Loss: 0.0034063705\n",
            "Loss: 0.0034314413\n",
            "Loss: 0.0034017507\n",
            "Loss: 0.0033940957\n",
            "Loss: 0.003385992\n",
            "Loss: 0.0033750155\n",
            "Loss: 0.0033595231\n",
            "Loss: 0.0033425528\n",
            "Loss: 0.0033253566\n",
            "Loss: 0.003311274\n",
            "Loss: 0.003295934\n",
            "Loss: 0.003294924\n",
            "Loss: 0.003282431\n",
            "Loss: 0.0032748408\n",
            "Loss: 0.0032647396\n",
            "Loss: 0.0032538404\n",
            "Loss: 0.0032456904\n",
            "Loss: 0.0032370612\n",
            "Loss: 0.0032308646\n",
            "Loss: 0.0032210303\n",
            "Loss: 0.0032056908\n",
            "Loss: 0.0031967736\n",
            "Loss: 0.0031865698\n",
            "Loss: 0.0031804354\n",
            "Loss: 0.0031707957\n",
            "Loss: 0.0032111248\n",
            "Loss: 0.003167138\n",
            "Loss: 0.0031554094\n",
            "Loss: 0.0031433704\n",
            "Loss: 0.0031335815\n",
            "Loss: 0.0031258543\n",
            "Loss: 0.0031206917\n",
            "Loss: 0.0031102346\n",
            "Loss: 0.0031034155\n",
            "Loss: 0.0030962136\n",
            "Loss: 0.0030895448\n",
            "Loss: 0.0030701\n",
            "Loss: 0.0030587348\n",
            "Loss: 0.003045811\n",
            "Loss: 0.0030366739\n",
            "Loss: 0.0030287057\n",
            "Loss: 0.0030229439\n",
            "Loss: 0.0030141813\n",
            "Loss: 0.0030047495\n",
            "Loss: 0.0029967837\n",
            "Loss: 0.002989446\n",
            "Loss: 0.0029826323\n",
            "Loss: 0.0029705341\n",
            "Loss: 0.0029646084\n",
            "Loss: 0.002947352\n",
            "Loss: 0.0029260674\n",
            "Loss: 0.002974838\n",
            "Loss: 0.0029210874\n",
            "Loss: 0.0029071798\n",
            "Loss: 0.0028992938\n",
            "Loss: 0.0028940088\n",
            "Loss: 0.0028883095\n",
            "Loss: 0.0028754177\n",
            "Loss: 0.0029524434\n",
            "Loss: 0.002873261\n",
            "Loss: 0.0028662968\n",
            "Loss: 0.0028582488\n",
            "Loss: 0.0028433097\n",
            "Loss: 0.00283812\n",
            "Loss: 0.002826475\n",
            "Loss: 0.0028168827\n",
            "Loss: 0.0028075124\n",
            "Loss: 0.0027971324\n",
            "Loss: 0.0027881803\n",
            "Loss: 0.0027816964\n",
            "Loss: 0.002774486\n",
            "Loss: 0.002768849\n",
            "Loss: 0.0027559346\n",
            "Loss: 0.0027684057\n",
            "Loss: 0.0027497523\n",
            "Loss: 0.0027423357\n",
            "Loss: 0.002737064\n",
            "Loss: 0.002731032\n",
            "Loss: 0.0027259877\n",
            "Loss: 0.002717613\n",
            "Loss: 0.0027139918\n",
            "Loss: 0.002709312\n",
            "Loss: 0.002710747\n",
            "Loss: 0.0027070055\n",
            "Loss: 0.0027027798\n",
            "Loss: 0.0026987835\n",
            "Loss: 0.002693195\n",
            "Loss: 0.0026865108\n",
            "Loss: 0.0026788132\n",
            "Loss: 0.0026753203\n",
            "Loss: 0.002670383\n",
            "Loss: 0.0026680853\n",
            "Loss: 0.0026649812\n",
            "Loss: 0.0026594698\n",
            "Loss: 0.0026486865\n",
            "Loss: 0.002708334\n",
            "Loss: 0.0026459189\n",
            "Loss: 0.0026375307\n",
            "Loss: 0.00263266\n",
            "Loss: 0.0026259143\n",
            "Loss: 0.0026187869\n",
            "Loss: 0.002611883\n",
            "Loss: 0.0026033428\n",
            "Loss: 0.0025960705\n",
            "Loss: 0.0025907313\n",
            "Loss: 0.0025834166\n",
            "Loss: 0.0025713346\n",
            "Loss: 0.0025575536\n",
            "Loss: 0.0025436406\n",
            "Loss: 0.0025364545\n",
            "Loss: 0.0025331995\n",
            "Loss: 0.0025293524\n",
            "Loss: 0.0025253065\n",
            "Loss: 0.0025096876\n",
            "Loss: 0.002676895\n",
            "Loss: 0.0025050426\n",
            "Loss: 0.0024980821\n",
            "Loss: 0.0024892811\n",
            "Loss: 0.0024796375\n",
            "Loss: 0.0024678423\n",
            "Loss: 0.0024590509\n",
            "Loss: 0.002450461\n",
            "Loss: 0.0024446582\n",
            "Loss: 0.0024402565\n",
            "Loss: 0.0024358328\n",
            "Loss: 0.002435678\n",
            "Loss: 0.0024323193\n",
            "Loss: 0.0024272206\n",
            "Loss: 0.0024220266\n",
            "Loss: 0.002417294\n",
            "Loss: 0.0024115616\n",
            "Loss: 0.0024286346\n",
            "Loss: 0.00240849\n",
            "Loss: 0.0024013403\n",
            "Loss: 0.0023925747\n",
            "Loss: 0.0023870883\n",
            "Loss: 0.00237958\n",
            "Loss: 0.0023707375\n",
            "Loss: 0.0023822885\n",
            "Loss: 0.002367605\n",
            "Loss: 0.0023631444\n",
            "Loss: 0.0023546463\n",
            "Loss: 0.002368307\n",
            "Loss: 0.0023524854\n",
            "Loss: 0.0023478945\n",
            "Loss: 0.0023441752\n",
            "Loss: 0.0023404516\n",
            "Loss: 0.002336855\n",
            "Loss: 0.0023361943\n",
            "Loss: 0.0023311903\n",
            "Loss: 0.0023288091\n",
            "Loss: 0.002326194\n",
            "Loss: 0.0023220815\n",
            "Loss: 0.0023157992\n",
            "Loss: 0.0023118625\n",
            "Loss: 0.002307291\n",
            "Loss: 0.002303078\n",
            "Loss: 0.0022986536\n",
            "Loss: 0.0022912002\n",
            "Loss: 0.0022860388\n",
            "Loss: 0.0022812388\n",
            "Loss: 0.00227714\n",
            "Loss: 0.002268623\n",
            "Loss: 0.0022651944\n",
            "Loss: 0.0022601155\n",
            "Loss: 0.0022584898\n",
            "Loss: 0.0022515394\n",
            "Loss: 0.0022459184\n",
            "Loss: 0.0022403896\n",
            "Loss: 0.0022372385\n",
            "Loss: 0.0022333707\n",
            "Loss: 0.0022290514\n",
            "Loss: 0.0022226698\n",
            "Loss: 0.002216443\n",
            "Loss: 0.0022138832\n",
            "Loss: 0.0022052275\n",
            "Loss: 0.0022017173\n",
            "Loss: 0.0021981813\n",
            "Loss: 0.002194221\n",
            "Loss: 0.0021889093\n",
            "Loss: 0.0021849873\n",
            "Loss: 0.0021815775\n",
            "Loss: 0.0021749595\n",
            "Loss: 0.0021764045\n",
            "Loss: 0.0021711076\n",
            "Loss: 0.0021645632\n",
            "Loss: 0.0021571387\n",
            "Loss: 0.002150956\n",
            "Loss: 0.0021446224\n",
            "Loss: 0.0021406107\n",
            "Loss: 0.0021361716\n",
            "Loss: 0.0021311024\n",
            "Loss: 0.0021235498\n",
            "Loss: 0.0021152836\n",
            "Loss: 0.0021133595\n",
            "Loss: 0.002105892\n",
            "Loss: 0.0021030398\n",
            "Loss: 0.0020987384\n",
            "Loss: 0.0020978001\n",
            "Loss: 0.00209036\n",
            "Loss: 0.0020859311\n",
            "Loss: 0.0020821362\n",
            "Loss: 0.0020776694\n",
            "Loss: 0.0020816412\n",
            "Loss: 0.0020746053\n",
            "Loss: 0.0020688875\n",
            "Loss: 0.0020641491\n",
            "Loss: 0.002057825\n",
            "Loss: 0.0020573288\n",
            "Loss: 0.0020548403\n",
            "Loss: 0.0020501232\n",
            "Loss: 0.0020454144\n",
            "Loss: 0.0020420838\n",
            "Loss: 0.0020362828\n",
            "Loss: 0.0020440887\n",
            "Loss: 0.0020334069\n",
            "Loss: 0.0020282227\n",
            "Loss: 0.0020256694\n",
            "Loss: 0.0020211232\n",
            "Loss: 0.0020158086\n",
            "Loss: 0.0020201653\n",
            "Loss: 0.0020127664\n",
            "Loss: 0.0020063045\n",
            "Loss: 0.0020002425\n",
            "Loss: 0.001993252\n",
            "Loss: 0.0019958378\n",
            "Loss: 0.0019899192\n",
            "Loss: 0.0019843315\n",
            "Loss: 0.0019767524\n",
            "Loss: 0.0019763533\n",
            "Loss: 0.0019741773\n",
            "Loss: 0.0019707156\n",
            "Loss: 0.001966462\n",
            "Loss: 0.0019600086\n",
            "Loss: 0.0019561192\n",
            "Loss: 0.0019501994\n",
            "Loss: 0.0019478786\n",
            "Loss: 0.0019436779\n",
            "Loss: 0.0019393864\n",
            "Loss: 0.0019339959\n",
            "Loss: 0.0019280533\n",
            "Loss: 0.0019254608\n",
            "Loss: 0.0019195899\n",
            "Loss: 0.0019156345\n",
            "Loss: 0.0019120041\n",
            "Loss: 0.001906296\n",
            "Loss: 0.0019092518\n",
            "Loss: 0.0019016508\n",
            "Loss: 0.001895848\n",
            "Loss: 0.0018916391\n",
            "Loss: 0.0018878012\n",
            "Loss: 0.0018780262\n",
            "Loss: 0.0018686734\n",
            "Loss: 0.001858909\n",
            "Loss: 0.0018547943\n",
            "Loss: 0.001850167\n",
            "Loss: 0.001845749\n",
            "Loss: 0.0018416868\n",
            "Loss: 0.0018353943\n",
            "Loss: 0.00183328\n",
            "Loss: 0.0018299103\n",
            "Loss: 0.001825281\n",
            "Loss: 0.0018194268\n",
            "Loss: 0.0018136916\n",
            "Loss: 0.0018026468\n",
            "Loss: 0.0017983667\n",
            "Loss: 0.0017949636\n",
            "Loss: 0.0017915652\n",
            "Loss: 0.0017851812\n",
            "Loss: 0.0017804198\n",
            "Loss: 0.0017936083\n",
            "Loss: 0.0017774563\n",
            "Loss: 0.0017710491\n",
            "Loss: 0.0017678804\n",
            "Loss: 0.0017644409\n",
            "Loss: 0.0017627261\n",
            "Loss: 0.001758923\n",
            "Loss: 0.0017530196\n",
            "Loss: 0.0017481141\n",
            "Loss: 0.0017419339\n",
            "Loss: 0.0017359778\n",
            "Loss: 0.0017331168\n",
            "Loss: 0.0017285027\n",
            "Loss: 0.0017255079\n",
            "Loss: 0.0017225873\n",
            "Loss: 0.0017189693\n",
            "Loss: 0.0017171743\n",
            "Loss: 0.0017124289\n",
            "Loss: 0.0017089972\n",
            "Loss: 0.0017050874\n",
            "Loss: 0.0017019621\n",
            "Loss: 0.0017036507\n",
            "Loss: 0.001700291\n",
            "Loss: 0.0016978811\n",
            "Loss: 0.001695598\n",
            "Loss: 0.0016924907\n",
            "Loss: 0.0016894271\n",
            "Loss: 0.0016869869\n",
            "Loss: 0.0016845886\n",
            "Loss: 0.0016810992\n",
            "Loss: 0.0016774123\n",
            "Loss: 0.0016751261\n",
            "Loss: 0.001669407\n",
            "Loss: 0.0016672593\n",
            "Loss: 0.0016633583\n",
            "Loss: 0.0016584713\n",
            "Loss: 0.0016536682\n",
            "Loss: 0.0016498587\n",
            "Loss: 0.0016467571\n",
            "Loss: 0.0016447271\n",
            "Loss: 0.0016421876\n",
            "Loss: 0.0016370609\n",
            "Loss: 0.0016367226\n",
            "Loss: 0.0016333912\n",
            "Loss: 0.0016264131\n",
            "Loss: 0.0016225446\n",
            "Loss: 0.0016198582\n",
            "Loss: 0.0016160485\n",
            "Loss: 0.0016090962\n",
            "Loss: 0.0016407348\n",
            "Loss: 0.0016075554\n",
            "Loss: 0.001604082\n",
            "Loss: 0.0016019195\n",
            "Loss: 0.0016003037\n",
            "Loss: 0.001597048\n",
            "Loss: 0.0015898998\n",
            "Loss: 0.0016013635\n",
            "Loss: 0.0015869404\n",
            "Loss: 0.0015787061\n",
            "Loss: 0.0015737684\n",
            "Loss: 0.0015690131\n",
            "Loss: 0.0015629838\n",
            "Loss: 0.0015595711\n",
            "Loss: 0.0015489631\n",
            "Loss: 0.0015443304\n",
            "Loss: 0.001536618\n",
            "Loss: 0.0015292168\n",
            "Loss: 0.0015212891\n",
            "Loss: 0.0015258205\n",
            "Loss: 0.0015175275\n",
            "Loss: 0.001513026\n",
            "Loss: 0.0015101554\n",
            "Loss: 0.0015061533\n",
            "Loss: 0.0015022934\n",
            "Loss: 0.0014989313\n",
            "Loss: 0.001495346\n",
            "Loss: 0.0014904474\n",
            "Loss: 0.0014875163\n",
            "Loss: 0.001484746\n",
            "Loss: 0.0014831547\n",
            "Loss: 0.0014808862\n",
            "Loss: 0.0014789322\n",
            "Loss: 0.0014747486\n",
            "Loss: 0.0014704078\n",
            "Loss: 0.0014639244\n",
            "Loss: 0.001457834\n",
            "Loss: 0.0014491599\n",
            "Loss: 0.0014444874\n",
            "Loss: 0.0014380482\n",
            "Loss: 0.0014341103\n",
            "Loss: 0.0014300048\n",
            "Loss: 0.001429806\n",
            "Loss: 0.0014265815\n",
            "Loss: 0.0014218418\n",
            "Loss: 0.0014188729\n",
            "Loss: 0.0014189197\n",
            "Loss: 0.0014172121\n",
            "Loss: 0.0014145691\n",
            "Loss: 0.0014126848\n",
            "Loss: 0.0014085617\n",
            "Loss: 0.0014113307\n",
            "Loss: 0.0014066761\n",
            "Loss: 0.0014038213\n",
            "Loss: 0.0014009578\n",
            "Loss: 0.0013972986\n",
            "Loss: 0.0013922798\n",
            "Loss: 0.001385896\n",
            "Loss: 0.0013789032\n",
            "Loss: 0.0013756142\n",
            "Loss: 0.001371878\n",
            "Loss: 0.0013698809\n",
            "Loss: 0.0013668382\n",
            "Loss: 0.0013610135\n",
            "Loss: 0.0013605643\n",
            "Loss: 0.0013581036\n",
            "Loss: 0.001353383\n",
            "Loss: 0.00135098\n",
            "Loss: 0.0013475002\n",
            "Loss: 0.0013446539\n",
            "Loss: 0.0013411979\n",
            "Loss: 0.0013406292\n",
            "Loss: 0.0013357691\n",
            "Loss: 0.0013344083\n",
            "Loss: 0.0013312267\n",
            "Loss: 0.0013277973\n",
            "Loss: 0.0013326779\n",
            "Loss: 0.001325925\n",
            "Loss: 0.0013232895\n",
            "Loss: 0.0013210804\n",
            "Loss: 0.001318925\n",
            "Loss: 0.0013150093\n",
            "Loss: 0.0013083813\n",
            "Loss: 0.0013010633\n",
            "Loss: 0.0013026881\n",
            "Loss: 0.0012972843\n",
            "Loss: 0.0012932845\n",
            "Loss: 0.0012913612\n",
            "Loss: 0.001289217\n",
            "Loss: 0.0012878497\n",
            "Loss: 0.0012865394\n",
            "Loss: 0.0012836994\n",
            "Loss: 0.0012798337\n",
            "Loss: 0.0012781558\n",
            "Loss: 0.0012756344\n",
            "Loss: 0.0012736843\n",
            "Loss: 0.0012721793\n",
            "Loss: 0.0012692192\n",
            "Loss: 0.0012663219\n",
            "Loss: 0.0012685591\n",
            "Loss: 0.0012647065\n",
            "Loss: 0.0012631654\n",
            "Loss: 0.0012608203\n",
            "Loss: 0.0012590275\n",
            "Loss: 0.0012564111\n",
            "Loss: 0.001253055\n",
            "Loss: 0.001251392\n",
            "Loss: 0.001248515\n",
            "Loss: 0.0012464485\n",
            "Loss: 0.0012433367\n",
            "Loss: 0.0012417652\n",
            "Loss: 0.0012392038\n",
            "Loss: 0.0012364928\n",
            "Loss: 0.0012334555\n",
            "Loss: 0.0012305384\n",
            "Loss: 0.0012284319\n",
            "Loss: 0.0012258866\n",
            "Loss: 0.001224214\n",
            "Loss: 0.0012207748\n",
            "Loss: 0.0012163663\n",
            "Loss: 0.001214135\n",
            "Loss: 0.0012100573\n",
            "Loss: 0.0012087042\n",
            "Loss: 0.0012077952\n",
            "Loss: 0.0012045398\n",
            "Loss: 0.0012010983\n",
            "Loss: 0.001195099\n",
            "Loss: 0.0011981835\n",
            "Loss: 0.001192271\n",
            "Loss: 0.0011889976\n",
            "Loss: 0.0011870099\n",
            "Loss: 0.0011851059\n",
            "Loss: 0.0011826768\n",
            "Loss: 0.0011807479\n",
            "Loss: 0.0011793631\n",
            "Loss: 0.0011774905\n",
            "Loss: 0.0011737638\n",
            "Loss: 0.001169258\n",
            "Loss: 0.0011653241\n",
            "Loss: 0.0011627314\n",
            "Loss: 0.0011614789\n",
            "Loss: 0.0011606133\n",
            "Loss: 0.0011585664\n",
            "Loss: 0.0011644755\n",
            "Loss: 0.0011576061\n",
            "Loss: 0.001155303\n",
            "Loss: 0.0011521056\n",
            "Loss: 0.0011496593\n",
            "Loss: 0.001146901\n",
            "Loss: 0.0011508585\n",
            "Loss: 0.0011455092\n",
            "Loss: 0.0011427402\n",
            "Loss: 0.0011397375\n",
            "Loss: 0.0011365933\n",
            "Loss: 0.0011352971\n",
            "Loss: 0.001133214\n",
            "Loss: 0.0011312146\n",
            "Loss: 0.0011291634\n",
            "Loss: 0.0011267432\n",
            "Loss: 0.0011235\n",
            "Loss: 0.0011904449\n",
            "Loss: 0.0011222477\n",
            "Loss: 0.0011190793\n",
            "Loss: 0.0011163957\n",
            "Loss: 0.0011149022\n",
            "Loss: 0.0011134144\n",
            "Loss: 0.0011113649\n",
            "Loss: 0.001108899\n",
            "Loss: 0.0011043563\n",
            "Loss: 0.0011031688\n",
            "Loss: 0.0011014767\n",
            "Loss: 0.001099237\n",
            "Loss: 0.0010970753\n",
            "Loss: 0.0010947213\n",
            "Loss: 0.0010909997\n",
            "Loss: 0.0010900472\n",
            "Loss: 0.0010869452\n",
            "Loss: 0.0010849816\n",
            "Loss: 0.0010823803\n",
            "Loss: 0.0010792636\n",
            "Loss: 0.0010836419\n",
            "Loss: 0.0010780091\n",
            "Loss: 0.0010757626\n",
            "Loss: 0.0010737793\n",
            "Loss: 0.0010711256\n",
            "Loss: 0.0010697222\n",
            "Loss: 0.0010676327\n",
            "Loss: 0.0010668766\n",
            "Loss: 0.0010661581\n",
            "Loss: 0.0010650199\n",
            "Loss: 0.0010630018\n",
            "Loss: 0.0010615846\n",
            "Loss: 0.0010604264\n",
            "Loss: 0.0010588743\n",
            "Loss: 0.0010573672\n",
            "Loss: 0.0010546786\n",
            "Loss: 0.0010530218\n",
            "Loss: 0.001050376\n",
            "Loss: 0.0010484718\n",
            "Loss: 0.0010461512\n",
            "Loss: 0.0010442685\n",
            "Loss: 0.001041377\n",
            "Loss: 0.0010394034\n",
            "Loss: 0.0010375212\n",
            "Loss: 0.0010351927\n",
            "Loss: 0.0010336764\n",
            "Loss: 0.0010318535\n",
            "Loss: 0.0010305415\n",
            "Loss: 0.0010297068\n",
            "Loss: 0.0010281166\n",
            "Loss: 0.0010255004\n",
            "Loss: 0.0010234269\n",
            "Loss: 0.0010215559\n",
            "Loss: 0.0010200967\n",
            "Loss: 0.001017674\n",
            "Loss: 0.0010161286\n",
            "Loss: 0.0010155307\n",
            "Loss: 0.0010143861\n",
            "Loss: 0.0010127996\n",
            "Loss: 0.0010105602\n",
            "Loss: 0.0010093898\n",
            "Loss: 0.0010081239\n",
            "Loss: 0.0010075329\n",
            "Loss: 0.001006703\n",
            "Loss: 0.0010043767\n",
            "Loss: 0.0010012685\n",
            "Loss: 0.0010057141\n",
            "Loss: 0.000999663\n",
            "Loss: 0.0009973344\n",
            "Loss: 0.0009955742\n",
            "Loss: 0.0009942336\n",
            "Loss: 0.0009934811\n",
            "Loss: 0.0009915526\n",
            "Loss: 0.0009900301\n",
            "Loss: 0.000989201\n",
            "Loss: 0.000987498\n",
            "Loss: 0.0010089924\n",
            "Loss: 0.0009867747\n",
            "Loss: 0.0009847749\n",
            "Loss: 0.0009832024\n",
            "Loss: 0.0009816323\n",
            "Loss: 0.0009802784\n",
            "Loss: 0.0009788222\n",
            "Loss: 0.00097594905\n",
            "Loss: 0.0009742569\n",
            "Loss: 0.0009718839\n",
            "Loss: 0.00096970325\n",
            "Loss: 0.0009675465\n",
            "Loss: 0.0009639921\n",
            "Loss: 0.0009600583\n",
            "Loss: 0.0009574748\n",
            "Loss: 0.0009562245\n",
            "Loss: 0.00095411367\n",
            "Loss: 0.0009530657\n",
            "Loss: 0.00095074705\n",
            "Loss: 0.0009492547\n",
            "Loss: 0.0009479722\n",
            "Loss: 0.00094623363\n",
            "Loss: 0.0009442776\n",
            "Loss: 0.00094249425\n",
            "Loss: 0.0009410352\n",
            "Loss: 0.00093884487\n",
            "Loss: 0.0009361574\n",
            "Loss: 0.00093432853\n",
            "Loss: 0.00093301537\n",
            "Loss: 0.0009319318\n",
            "Loss: 0.000930604\n",
            "Loss: 0.00092852325\n",
            "Loss: 0.00092691364\n",
            "Loss: 0.0009259042\n",
            "Loss: 0.00092505803\n",
            "Loss: 0.0009244562\n",
            "Loss: 0.0009233289\n",
            "Loss: 0.0009218615\n",
            "Loss: 0.0009198573\n",
            "Loss: 0.00091889343\n",
            "Loss: 0.00091648934\n",
            "Loss: 0.0009156935\n",
            "Loss: 0.00091400585\n",
            "Loss: 0.0009122684\n",
            "Loss: 0.0009093954\n",
            "Loss: 0.0009056833\n",
            "Loss: 0.0009038232\n",
            "Loss: 0.0009010647\n",
            "Loss: 0.000899605\n",
            "Loss: 0.00089703116\n",
            "Loss: 0.000894836\n",
            "Loss: 0.0008927842\n",
            "Loss: 0.0008896168\n",
            "Loss: 0.00089042773\n",
            "Loss: 0.0008886468\n",
            "Loss: 0.0008870645\n",
            "Loss: 0.0008859817\n",
            "Loss: 0.00088403735\n",
            "Loss: 0.00088319543\n",
            "Loss: 0.00088047335\n",
            "Loss: 0.0008789593\n",
            "Loss: 0.00087746495\n",
            "Loss: 0.0008743346\n",
            "Loss: 0.00087556115\n",
            "Loss: 0.00087278965\n",
            "Loss: 0.0008705206\n",
            "Loss: 0.0008693116\n",
            "Loss: 0.00086633477\n",
            "Loss: 0.00086411694\n",
            "Loss: 0.0008614283\n",
            "Loss: 0.00085891504\n",
            "Loss: 0.0008557709\n",
            "Loss: 0.00085440185\n",
            "Loss: 0.00085393107\n",
            "Loss: 0.00085218134\n",
            "Loss: 0.0008512868\n",
            "Loss: 0.0008499323\n",
            "Loss: 0.00084780506\n",
            "Loss: 0.0008471643\n",
            "Loss: 0.0008434192\n",
            "Loss: 0.00084217393\n",
            "Loss: 0.00084060326\n",
            "Loss: 0.00083853607\n",
            "Loss: 0.0008372581\n",
            "Loss: 0.00083448435\n",
            "Loss: 0.0008333979\n",
            "Loss: 0.0008319254\n",
            "Loss: 0.0008303446\n",
            "Loss: 0.0008286191\n",
            "Loss: 0.0008273579\n",
            "Loss: 0.0008264771\n",
            "Loss: 0.00082524575\n",
            "Loss: 0.0008229422\n",
            "Loss: 0.0008212362\n",
            "Loss: 0.0008194146\n",
            "Loss: 0.0008181253\n",
            "Loss: 0.0008162244\n",
            "Loss: 0.000814492\n",
            "Loss: 0.0008120955\n",
            "Loss: 0.00081035146\n",
            "Loss: 0.00080799416\n",
            "Loss: 0.00080560613\n",
            "Loss: 0.0008044587\n",
            "Loss: 0.00080340705\n",
            "Loss: 0.0008024284\n",
            "Loss: 0.0008011135\n",
            "Loss: 0.00079897174\n",
            "Loss: 0.0007967036\n",
            "Loss: 0.0007955452\n",
            "Loss: 0.0007941326\n",
            "Loss: 0.0007934112\n",
            "Loss: 0.00079215295\n",
            "Loss: 0.0007947406\n",
            "Loss: 0.00079144456\n",
            "Loss: 0.0007896348\n",
            "Loss: 0.00078844174\n",
            "Loss: 0.00078667165\n",
            "Loss: 0.0007858874\n",
            "Loss: 0.00078459125\n",
            "Loss: 0.0007830687\n",
            "Loss: 0.00078179094\n",
            "Loss: 0.00078038115\n",
            "Loss: 0.0007778945\n",
            "Loss: 0.0007754605\n",
            "Loss: 0.00077322894\n",
            "Loss: 0.00076989795\n",
            "Loss: 0.00077062985\n",
            "Loss: 0.0007678055\n",
            "Loss: 0.0007640276\n",
            "Loss: 0.000761551\n",
            "Loss: 0.0007585437\n",
            "Loss: 0.0007549394\n",
            "Loss: 0.00075949926\n",
            "Loss: 0.0007533941\n",
            "Loss: 0.00075011386\n",
            "Loss: 0.0007476698\n",
            "Loss: 0.0007447726\n",
            "Loss: 0.0007425783\n",
            "Loss: 0.00074062066\n",
            "Loss: 0.0007388521\n",
            "Loss: 0.00073707855\n",
            "Loss: 0.00073553866\n",
            "Loss: 0.0007320276\n",
            "Loss: 0.0007313293\n",
            "Loss: 0.00072807894\n",
            "Loss: 0.0007274975\n",
            "Loss: 0.00072580506\n",
            "Loss: 0.0007229982\n",
            "Loss: 0.0007221177\n",
            "Loss: 0.00071879954\n",
            "Loss: 0.0007178673\n",
            "Loss: 0.0007170007\n",
            "Loss: 0.0007157079\n",
            "Loss: 0.00071363675\n",
            "Loss: 0.00071039237\n",
            "Loss: 0.0007085562\n",
            "Loss: 0.0007063559\n",
            "Loss: 0.00070581806\n",
            "Loss: 0.0007040736\n",
            "Loss: 0.0007020853\n",
            "Loss: 0.000700537\n",
            "Loss: 0.00069844234\n",
            "Loss: 0.00069740927\n",
            "Loss: 0.0006959955\n",
            "Loss: 0.0006951054\n",
            "Loss: 0.0006935139\n",
            "Loss: 0.0006920467\n",
            "Loss: 0.00069053797\n",
            "Loss: 0.0006881048\n",
            "Loss: 0.0006869766\n",
            "Loss: 0.0006854632\n",
            "Loss: 0.000684264\n",
            "Loss: 0.0006838351\n",
            "Loss: 0.0006830258\n",
            "Loss: 0.00068220403\n",
            "Loss: 0.00068081263\n",
            "Loss: 0.00067957595\n",
            "Loss: 0.0006788309\n",
            "Loss: 0.00067742483\n",
            "Loss: 0.00068049994\n",
            "Loss: 0.0006766728\n",
            "Loss: 0.0006755993\n",
            "Loss: 0.0006747489\n",
            "Loss: 0.0006740801\n",
            "Loss: 0.0006738292\n",
            "Loss: 0.0006717269\n",
            "Loss: 0.0006707674\n",
            "Loss: 0.00066965877\n",
            "Loss: 0.0006689641\n",
            "Loss: 0.0006681938\n",
            "Loss: 0.0006664011\n",
            "Loss: 0.0006650535\n",
            "Loss: 0.0006641295\n",
            "Loss: 0.00066239096\n",
            "Loss: 0.0006614283\n",
            "Loss: 0.00065972377\n",
            "Loss: 0.0006600533\n",
            "Loss: 0.0006592766\n",
            "Loss: 0.00065861957\n",
            "Loss: 0.00065757136\n",
            "Loss: 0.0006563015\n",
            "Loss: 0.000654708\n",
            "Loss: 0.0006539973\n",
            "Loss: 0.0006517612\n",
            "Loss: 0.00065096\n",
            "Loss: 0.0006497323\n",
            "Loss: 0.00064827193\n",
            "Loss: 0.00064585987\n",
            "Loss: 0.0006447761\n",
            "Loss: 0.0006434049\n",
            "Loss: 0.00064232456\n",
            "Loss: 0.00064118963\n",
            "Loss: 0.0006404577\n",
            "Loss: 0.0006382569\n",
            "Loss: 0.00063723937\n",
            "Loss: 0.0006359437\n",
            "Loss: 0.0006349655\n",
            "Loss: 0.00063366594\n",
            "Loss: 0.0006323003\n",
            "Loss: 0.0006313868\n",
            "Loss: 0.0006304432\n",
            "Loss: 0.0006295815\n",
            "Loss: 0.0006283922\n",
            "Loss: 0.0006273527\n",
            "Loss: 0.0006385657\n",
            "Loss: 0.00062686874\n",
            "Loss: 0.0006255171\n",
            "Loss: 0.00062469585\n",
            "Loss: 0.00062377314\n",
            "Loss: 0.0006226647\n",
            "Loss: 0.000621211\n",
            "Loss: 0.00061965303\n",
            "Loss: 0.0006184906\n",
            "Loss: 0.0006171431\n",
            "Loss: 0.0006164087\n",
            "Loss: 0.0006155681\n",
            "Loss: 0.0006149716\n",
            "Loss: 0.0006143531\n",
            "Loss: 0.00061363523\n",
            "Loss: 0.0006125058\n",
            "Loss: 0.0006127207\n",
            "Loss: 0.0006118652\n",
            "Loss: 0.00061105815\n",
            "Loss: 0.000609992\n",
            "Loss: 0.000609261\n",
            "Loss: 0.0006082244\n",
            "Loss: 0.0006074082\n",
            "Loss: 0.00060662464\n",
            "Loss: 0.00060541276\n",
            "Loss: 0.0006039989\n",
            "Loss: 0.0006028078\n",
            "Loss: 0.00060185546\n",
            "Loss: 0.00060124067\n",
            "Loss: 0.0006005934\n",
            "Loss: 0.00059962\n",
            "Loss: 0.0005982178\n",
            "Loss: 0.00059626845\n",
            "Loss: 0.0006033055\n",
            "Loss: 0.0005958369\n",
            "Loss: 0.00059479347\n",
            "Loss: 0.00059412007\n",
            "Loss: 0.0005931801\n",
            "Loss: 0.00059182534\n",
            "Loss: 0.00059093995\n",
            "Loss: 0.000589462\n",
            "Loss: 0.00058890233\n",
            "Loss: 0.00058829243\n",
            "Loss: 0.0005871019\n",
            "Loss: 0.00058708794\n",
            "Loss: 0.00058649655\n",
            "Loss: 0.00058564\n",
            "Loss: 0.0005852541\n",
            "Loss: 0.0005845049\n",
            "Loss: 0.0005839972\n",
            "Loss: 0.00058288523\n",
            "Loss: 0.00058204174\n",
            "Loss: 0.0005822367\n",
            "Loss: 0.0005814938\n",
            "Loss: 0.00058075204\n",
            "Loss: 0.0005799589\n",
            "Loss: 0.00057887775\n",
            "Loss: 0.00057961536\n",
            "Loss: 0.00057851436\n",
            "Loss: 0.0005777449\n",
            "Loss: 0.0005771951\n",
            "Loss: 0.000576552\n",
            "Loss: 0.0005756962\n",
            "Loss: 0.0005742679\n",
            "Loss: 0.0005796832\n",
            "Loss: 0.0005738133\n",
            "Loss: 0.00057281007\n",
            "Loss: 0.0005719726\n",
            "Loss: 0.00057093066\n",
            "Loss: 0.0005717872\n",
            "Loss: 0.00057044375\n",
            "Loss: 0.00056982075\n",
            "Loss: 0.00056912436\n",
            "Loss: 0.00056868116\n",
            "Loss: 0.0005677251\n",
            "Loss: 0.0005719825\n",
            "Loss: 0.000567409\n",
            "Loss: 0.0005664757\n",
            "Loss: 0.00056546496\n",
            "Loss: 0.0005645784\n",
            "Loss: 0.0005638583\n",
            "Loss: 0.0005629213\n",
            "Loss: 0.0005620413\n",
            "Loss: 0.0005609171\n",
            "Loss: 0.0005599937\n",
            "Loss: 0.00055937574\n",
            "Loss: 0.0005586416\n",
            "Loss: 0.00055814476\n",
            "Loss: 0.0005576225\n",
            "Loss: 0.0005565445\n",
            "Loss: 0.0005555548\n",
            "Loss: 0.0005547204\n",
            "Loss: 0.00055323617\n",
            "Loss: 0.00055491936\n",
            "Loss: 0.00055289146\n",
            "Loss: 0.000552377\n",
            "Loss: 0.0005509262\n",
            "Loss: 0.000549818\n",
            "Loss: 0.0005478256\n",
            "Loss: 0.0005590092\n",
            "Loss: 0.00054741045\n",
            "Loss: 0.00054640946\n",
            "Loss: 0.0005458585\n",
            "Loss: 0.0005451415\n",
            "Loss: 0.0005443251\n",
            "Loss: 0.00054422603\n",
            "Loss: 0.0005436593\n",
            "Loss: 0.0005429183\n",
            "Loss: 0.0005422436\n",
            "Loss: 0.00054162845\n",
            "Loss: 0.00054037734\n",
            "Loss: 0.0005398389\n",
            "Loss: 0.0005386727\n",
            "Loss: 0.0005380934\n",
            "Loss: 0.0005376585\n",
            "Loss: 0.0005372355\n",
            "Loss: 0.0005368731\n",
            "Loss: 0.0005364\n",
            "Loss: 0.00053594547\n",
            "Loss: 0.00053482974\n",
            "Loss: 0.00053359056\n",
            "Loss: 0.0005322983\n",
            "Loss: 0.0005330868\n",
            "Loss: 0.0005318599\n",
            "Loss: 0.0005311837\n",
            "Loss: 0.00053047284\n",
            "Loss: 0.0005299244\n",
            "Loss: 0.0005288987\n",
            "Loss: 0.00053234096\n",
            "Loss: 0.0005286505\n",
            "Loss: 0.0005279722\n",
            "Loss: 0.0005274348\n",
            "Loss: 0.0005265206\n",
            "Loss: 0.000526753\n",
            "Loss: 0.00052605185\n",
            "Loss: 0.00052511983\n",
            "Loss: 0.0005244232\n",
            "Loss: 0.0005236099\n",
            "Loss: 0.000522534\n",
            "Loss: 0.0005229718\n",
            "Loss: 0.0005218158\n",
            "Loss: 0.0005208686\n",
            "Loss: 0.0005199339\n",
            "Loss: 0.0005190995\n",
            "Loss: 0.0005192379\n",
            "Loss: 0.00051866093\n",
            "Loss: 0.0005182928\n",
            "Loss: 0.0005177284\n",
            "Loss: 0.0005171765\n",
            "Loss: 0.0005155682\n",
            "Loss: 0.0005137083\n",
            "Loss: 0.0005132855\n",
            "Loss: 0.0005113528\n",
            "Loss: 0.00051097566\n",
            "Loss: 0.0005105784\n",
            "Loss: 0.0005101081\n",
            "Loss: 0.0005097093\n",
            "Loss: 0.00050904846\n",
            "Loss: 0.00050837325\n",
            "Loss: 0.0005104841\n",
            "Loss: 0.00050771877\n",
            "Loss: 0.0005066309\n",
            "Loss: 0.0005060241\n",
            "Loss: 0.00050513836\n",
            "Loss: 0.00050432223\n",
            "Loss: 0.0005032644\n",
            "Loss: 0.00050250546\n",
            "Loss: 0.00050122716\n",
            "Loss: 0.00050035573\n",
            "Loss: 0.00049869664\n",
            "Loss: 0.0004970209\n",
            "Loss: 0.000496086\n",
            "Loss: 0.0004949204\n",
            "Loss: 0.00049436593\n",
            "Loss: 0.0004937314\n",
            "Loss: 0.0004928817\n",
            "Loss: 0.0004916928\n",
            "Loss: 0.0004927047\n",
            "Loss: 0.00049112784\n",
            "Loss: 0.00049001013\n",
            "Loss: 0.00048912293\n",
            "Loss: 0.00048823073\n",
            "Loss: 0.0004894263\n",
            "Loss: 0.00048792397\n",
            "Loss: 0.00048722874\n",
            "Loss: 0.00048631613\n",
            "Loss: 0.0004853455\n",
            "Loss: 0.0004845287\n",
            "Loss: 0.00048383322\n",
            "Loss: 0.00048327545\n",
            "Loss: 0.00048267812\n",
            "Loss: 0.00048204567\n",
            "Loss: 0.00048126152\n",
            "Loss: 0.00047998084\n",
            "Loss: 0.00047935414\n",
            "Loss: 0.00047844346\n",
            "Loss: 0.00047812302\n",
            "Loss: 0.00047730916\n",
            "Loss: 0.0004766737\n",
            "Loss: 0.00047605857\n",
            "Loss: 0.00047560432\n",
            "Loss: 0.00047503895\n",
            "Loss: 0.00047456785\n",
            "Loss: 0.0004739173\n",
            "Loss: 0.00047346848\n",
            "Loss: 0.0004724078\n",
            "Loss: 0.00047187763\n",
            "Loss: 0.00047082212\n",
            "Loss: 0.00047021225\n",
            "Loss: 0.00046922063\n",
            "Loss: 0.0004715794\n",
            "Loss: 0.00046887653\n",
            "Loss: 0.00046855013\n",
            "Loss: 0.00046817097\n",
            "Loss: 0.00046791762\n",
            "Loss: 0.000467124\n",
            "Loss: 0.0004683684\n",
            "Loss: 0.00046684407\n",
            "Loss: 0.00046624788\n",
            "Loss: 0.0004656943\n",
            "Loss: 0.0004652291\n",
            "Loss: 0.00046487013\n",
            "Loss: 0.0004642326\n",
            "Loss: 0.00046379928\n",
            "Loss: 0.00046339666\n",
            "Loss: 0.00046291226\n",
            "Loss: 0.00046176114\n",
            "Loss: 0.00046565244\n",
            "Loss: 0.0004615122\n",
            "Loss: 0.00046081218\n",
            "Loss: 0.00046033112\n",
            "Loss: 0.00045970085\n",
            "Loss: 0.00045895315\n",
            "Loss: 0.00045842706\n",
            "Loss: 0.00045727132\n",
            "Loss: 0.00045651465\n",
            "Loss: 0.00045572832\n",
            "Loss: 0.00045501394\n",
            "Loss: 0.00045452168\n",
            "Loss: 0.00045395183\n",
            "Loss: 0.0004535482\n",
            "Loss: 0.00045302787\n",
            "Loss: 0.0004518285\n",
            "Loss: 0.00045186153\n",
            "Loss: 0.00045138085\n",
            "Loss: 0.00045056886\n",
            "Loss: 0.00044992898\n",
            "Loss: 0.00044956245\n",
            "Loss: 0.00044887594\n",
            "Loss: 0.00045483763\n",
            "Loss: 0.0004486197\n",
            "Loss: 0.00044789803\n",
            "Loss: 0.000447379\n",
            "Loss: 0.00044689624\n",
            "Loss: 0.00044621236\n",
            "Loss: 0.00044518488\n",
            "Loss: 0.00044441124\n",
            "Loss: 0.00044397233\n",
            "Loss: 0.0004435575\n",
            "Loss: 0.00044334144\n",
            "Loss: 0.00044300937\n",
            "Loss: 0.00044218407\n",
            "Loss: 0.00044117522\n",
            "Loss: 0.00044083426\n",
            "Loss: 0.00043947974\n",
            "Loss: 0.00043888518\n",
            "Loss: 0.000438278\n",
            "Loss: 0.0004398313\n",
            "Loss: 0.00043802193\n",
            "Loss: 0.00043735973\n",
            "Loss: 0.00043677035\n",
            "Loss: 0.00043625268\n",
            "Loss: 0.00043580576\n",
            "Loss: 0.0004349128\n",
            "Loss: 0.0004343975\n",
            "Loss: 0.00043369213\n",
            "Loss: 0.0004331758\n",
            "Loss: 0.00043274043\n",
            "Loss: 0.00043207675\n",
            "Loss: 0.00043831865\n",
            "Loss: 0.00043172223\n",
            "Loss: 0.0004309756\n",
            "Loss: 0.0004302582\n",
            "Loss: 0.00042964704\n",
            "Loss: 0.0004289391\n",
            "Loss: 0.00042887928\n",
            "Loss: 0.00042841886\n",
            "Loss: 0.00042782607\n",
            "Loss: 0.00042717584\n",
            "Loss: 0.00042670185\n",
            "Loss: 0.00042571983\n",
            "Loss: 0.0004242844\n",
            "Loss: 0.00042315398\n",
            "Loss: 0.00042172085\n",
            "Loss: 0.0004210855\n",
            "Loss: 0.0004204351\n",
            "Loss: 0.00041949807\n",
            "Loss: 0.00042148074\n",
            "Loss: 0.00041909498\n",
            "Loss: 0.0004182795\n",
            "Loss: 0.00041759489\n",
            "Loss: 0.00041696464\n",
            "Loss: 0.00041666476\n",
            "Loss: 0.00041625847\n",
            "Loss: 0.00041589251\n",
            "Loss: 0.0004151663\n",
            "Loss: 0.0004146156\n",
            "Loss: 0.00041370472\n",
            "Loss: 0.00041350492\n",
            "Loss: 0.00041295978\n",
            "Loss: 0.000412112\n",
            "Loss: 0.0004129972\n",
            "Loss: 0.00041185095\n",
            "Loss: 0.00041144234\n",
            "Loss: 0.0004114096\n",
            "Loss: 0.0004111384\n",
            "Loss: 0.00041081075\n",
            "Loss: 0.00041035714\n",
            "Loss: 0.0004097895\n",
            "Loss: 0.0004085024\n",
            "Loss: 0.0004073743\n",
            "Loss: 0.00040650362\n",
            "Loss: 0.00040586726\n",
            "Loss: 0.00040535288\n",
            "Loss: 0.00040489825\n",
            "Loss: 0.00040451586\n",
            "Loss: 0.00040416827\n",
            "Loss: 0.0004039465\n",
            "Loss: 0.00040341588\n",
            "Loss: 0.00040250964\n",
            "Loss: 0.00040164313\n",
            "Loss: 0.00040114962\n",
            "Loss: 0.00040062465\n",
            "Loss: 0.0004002282\n",
            "Loss: 0.0003993645\n",
            "Loss: 0.00039834046\n",
            "Loss: 0.00040375092\n",
            "Loss: 0.00039806537\n",
            "Loss: 0.00039744523\n",
            "Loss: 0.00039680884\n",
            "Loss: 0.0003961455\n",
            "Loss: 0.00039532504\n",
            "Loss: 0.00039514518\n",
            "Loss: 0.0003944874\n",
            "Loss: 0.00039428973\n",
            "Loss: 0.00039400088\n",
            "Loss: 0.00039356953\n",
            "Loss: 0.00039304583\n",
            "Loss: 0.00039250986\n",
            "Loss: 0.000392028\n",
            "Loss: 0.0003915504\n",
            "Loss: 0.00039134006\n",
            "Loss: 0.00039063935\n",
            "Loss: 0.000390341\n",
            "Loss: 0.00038977579\n",
            "Loss: 0.0003893345\n",
            "Loss: 0.0003889096\n",
            "Loss: 0.00038833532\n",
            "Loss: 0.00038784728\n",
            "Loss: 0.00038719503\n",
            "Loss: 0.0003864041\n",
            "Loss: 0.00038651383\n",
            "Loss: 0.00038593536\n",
            "Loss: 0.00038549988\n",
            "Loss: 0.00038504036\n",
            "Loss: 0.00038464856\n",
            "Loss: 0.00038395013\n",
            "Loss: 0.00038323467\n",
            "Loss: 0.00038257163\n",
            "Loss: 0.0003815689\n",
            "Loss: 0.0003812056\n",
            "Loss: 0.00038066928\n",
            "Loss: 0.00038010516\n",
            "Loss: 0.00037912026\n",
            "Loss: 0.00038177823\n",
            "Loss: 0.00037867026\n",
            "Loss: 0.00037775442\n",
            "Loss: 0.00037689536\n",
            "Loss: 0.0003763152\n",
            "Loss: 0.00037572446\n",
            "Loss: 0.0003752362\n",
            "Loss: 0.0003745432\n",
            "Loss: 0.00037424493\n",
            "Loss: 0.00037388084\n",
            "Loss: 0.00037331035\n",
            "Loss: 0.00037243613\n",
            "Loss: 0.00037202437\n",
            "Loss: 0.0003708785\n",
            "Loss: 0.00037025375\n",
            "Loss: 0.00036948625\n",
            "Loss: 0.00036883494\n",
            "Loss: 0.0003680848\n",
            "Loss: 0.00036747966\n",
            "Loss: 0.00036689272\n",
            "Loss: 0.00036596644\n",
            "Loss: 0.0003652299\n",
            "Loss: 0.00036475487\n",
            "Loss: 0.0003644036\n",
            "Loss: 0.00036409046\n",
            "Loss: 0.0003636558\n",
            "Loss: 0.00036288705\n",
            "Loss: 0.00036512187\n",
            "Loss: 0.00036264717\n",
            "Loss: 0.00036209816\n",
            "Loss: 0.0003617017\n",
            "Loss: 0.000361275\n",
            "Loss: 0.00036076148\n",
            "Loss: 0.0003605768\n",
            "Loss: 0.00035985722\n",
            "Loss: 0.0003595964\n",
            "Loss: 0.000359288\n",
            "Loss: 0.00035886536\n",
            "Loss: 0.00035856362\n",
            "Loss: 0.0003580526\n",
            "Loss: 0.00035784068\n",
            "Loss: 0.0003574228\n",
            "Loss: 0.00035820773\n",
            "Loss: 0.00035727042\n",
            "Loss: 0.00035704015\n",
            "Loss: 0.0003566699\n",
            "Loss: 0.00035623577\n",
            "Loss: 0.00035573274\n",
            "Loss: 0.0003553009\n",
            "Loss: 0.00035500526\n",
            "Loss: 0.0003545254\n",
            "Loss: 0.00035427982\n",
            "Loss: 0.00035368177\n",
            "Loss: 0.00035325042\n",
            "Loss: 0.00035229587\n",
            "Loss: 0.0003519908\n",
            "Loss: 0.00035104156\n",
            "Loss: 0.00035060628\n",
            "Loss: 0.00035006728\n",
            "Loss: 0.00034943013\n",
            "Loss: 0.00034982385\n",
            "Loss: 0.000349039\n",
            "Loss: 0.00034820326\n",
            "Loss: 0.00034770987\n",
            "Loss: 0.0003468892\n",
            "Loss: 0.0003459205\n",
            "Loss: 0.00034753967\n",
            "Loss: 0.00034526532\n",
            "Loss: 0.00034412125\n",
            "Loss: 0.00034334496\n",
            "Loss: 0.0003445491\n",
            "Loss: 0.00034302624\n",
            "Loss: 0.00034254184\n",
            "Loss: 0.00034230395\n",
            "Loss: 0.00034179882\n",
            "Loss: 0.00034113406\n",
            "Loss: 0.00034019147\n",
            "Loss: 0.0003393175\n",
            "Loss: 0.00033875057\n",
            "Loss: 0.00033837193\n",
            "Loss: 0.00033803517\n",
            "Loss: 0.00033774826\n",
            "Loss: 0.00033684098\n",
            "Loss: 0.00033633964\n",
            "Loss: 0.00033580235\n",
            "Loss: 0.0003352603\n",
            "Loss: 0.0003350087\n",
            "Loss: 0.00033446384\n",
            "Loss: 0.00033410807\n",
            "Loss: 0.000333593\n",
            "Loss: 0.00033463672\n",
            "Loss: 0.00033339663\n",
            "Loss: 0.00033287128\n",
            "Loss: 0.00033233775\n",
            "Loss: 0.0003317049\n",
            "Loss: 0.00033122618\n",
            "Loss: 0.0003314049\n",
            "Loss: 0.00033089585\n",
            "Loss: 0.00033033596\n",
            "Loss: 0.00032990624\n",
            "Loss: 0.0003291226\n",
            "Loss: 0.0003289532\n",
            "Loss: 0.00032815506\n",
            "Loss: 0.00032783757\n",
            "Loss: 0.000327394\n",
            "Loss: 0.00032697042\n",
            "Loss: 0.0003265214\n",
            "Loss: 0.00032608758\n",
            "Loss: 0.00032615341\n",
            "Loss: 0.0003258293\n",
            "Loss: 0.00032553857\n",
            "Loss: 0.00032538993\n",
            "Loss: 0.00032493548\n",
            "Loss: 0.0003244608\n",
            "Loss: 0.00032385625\n",
            "Loss: 0.00032337225\n",
            "Loss: 0.0003230513\n",
            "Loss: 0.00032274402\n",
            "Loss: 0.00032244943\n",
            "Loss: 0.00032286748\n",
            "Loss: 0.00032214227\n",
            "Loss: 0.00032170519\n",
            "Loss: 0.00032128463\n",
            "Loss: 0.00032110314\n",
            "Loss: 0.00032062046\n",
            "Loss: 0.000320383\n",
            "Loss: 0.00031969475\n",
            "Loss: 0.00031934833\n",
            "Loss: 0.00031903817\n",
            "Loss: 0.00031968945\n",
            "Loss: 0.0003189047\n",
            "Loss: 0.0003186628\n",
            "Loss: 0.00031828907\n",
            "Loss: 0.00031794788\n",
            "Loss: 0.00031752355\n",
            "Loss: 0.00031717954\n",
            "Loss: 0.00031676318\n",
            "Loss: 0.00031618864\n",
            "Loss: 0.00031577714\n",
            "Loss: 0.00031524993\n",
            "Loss: 0.00031477746\n",
            "Loss: 0.00031433423\n",
            "Loss: 0.0003139223\n",
            "Loss: 0.00031386616\n",
            "Loss: 0.0003135252\n",
            "Loss: 0.00031303632\n",
            "Loss: 0.0003127334\n",
            "Loss: 0.00031230313\n",
            "Loss: 0.00031176518\n",
            "Loss: 0.00031231006\n",
            "Loss: 0.00031139623\n",
            "Loss: 0.00031079736\n",
            "Loss: 0.00031006272\n",
            "Loss: 0.00030961103\n",
            "Loss: 0.00030879828\n",
            "Loss: 0.00031015635\n",
            "Loss: 0.0003084764\n",
            "Loss: 0.00030794108\n",
            "Loss: 0.0003074815\n",
            "Loss: 0.0003069948\n",
            "Loss: 0.00030647096\n",
            "Loss: 0.00030640833\n",
            "Loss: 0.00030605285\n",
            "Loss: 0.00030554307\n",
            "Loss: 0.0003048439\n",
            "Loss: 0.00030442284\n",
            "Loss: 0.00030345452\n",
            "Loss: 0.00030603667\n",
            "Loss: 0.0003032528\n",
            "Loss: 0.00030275527\n",
            "Loss: 0.00030235873\n",
            "Loss: 0.0003018276\n",
            "Loss: 0.00030124933\n",
            "Loss: 0.00030152782\n",
            "Loss: 0.00030093218\n",
            "Loss: 0.0003005486\n",
            "Loss: 0.00030001943\n",
            "Loss: 0.00029936805\n",
            "Loss: 0.0002989248\n",
            "Loss: 0.00029880472\n",
            "Loss: 0.00029843498\n",
            "Loss: 0.00029824895\n",
            "Loss: 0.00029803603\n",
            "Loss: 0.0002977132\n",
            "Loss: 0.00029696675\n",
            "Loss: 0.00029653794\n",
            "Loss: 0.00029547673\n",
            "Loss: 0.00029510917\n",
            "Loss: 0.0002946178\n",
            "Loss: 0.00029510978\n",
            "Loss: 0.00029437\n",
            "Loss: 0.00029406275\n",
            "Loss: 0.00029359222\n",
            "Loss: 0.00029323692\n",
            "Loss: 0.00029252097\n",
            "Loss: 0.00029357895\n",
            "Loss: 0.00029226343\n",
            "Loss: 0.0002918826\n",
            "Loss: 0.00029154538\n",
            "Loss: 0.00029115452\n",
            "Loss: 0.00029071968\n",
            "Loss: 0.0002904713\n",
            "Loss: 0.00029022663\n",
            "Loss: 0.00029005832\n",
            "Loss: 0.00028974257\n",
            "Loss: 0.00028881239\n",
            "Loss: 0.00028839384\n",
            "Loss: 0.000287548\n",
            "Loss: 0.00028716627\n",
            "Loss: 0.00028677413\n",
            "Loss: 0.00028629485\n",
            "Loss: 0.00028570739\n",
            "Loss: 0.00028500712\n",
            "Loss: 0.00028431436\n",
            "Loss: 0.00028394122\n",
            "Loss: 0.00028352576\n",
            "Loss: 0.00028320105\n",
            "Loss: 0.00028300763\n",
            "Loss: 0.0002825573\n",
            "Loss: 0.00028261682\n",
            "Loss: 0.00028228702\n",
            "Loss: 0.00028176798\n",
            "Loss: 0.0002813725\n",
            "Loss: 0.00028089792\n",
            "Loss: 0.00028089073\n",
            "Loss: 0.0002807179\n",
            "Loss: 0.00028035973\n",
            "Loss: 0.0002799451\n",
            "Loss: 0.00027951648\n",
            "Loss: 0.0002789853\n",
            "Loss: 0.0002787832\n",
            "Loss: 0.00027827267\n",
            "Loss: 0.00027812755\n",
            "Loss: 0.0002779148\n",
            "Loss: 0.00027755462\n",
            "Loss: 0.00027718244\n",
            "Loss: 0.00027688715\n",
            "Loss: 0.00027670918\n",
            "Loss: 0.00027654908\n",
            "Loss: 0.00027642463\n",
            "Loss: 0.00027610493\n",
            "Loss: 0.00027568702\n",
            "Loss: 0.0002751056\n",
            "Loss: 0.00027445762\n",
            "Loss: 0.00027412438\n",
            "Loss: 0.00027388966\n",
            "Loss: 0.00027373873\n",
            "Loss: 0.0002735773\n",
            "Loss: 0.00027315534\n",
            "Loss: 0.00027295208\n",
            "Loss: 0.00027340764\n",
            "Loss: 0.000272737\n",
            "Loss: 0.00027225967\n",
            "Loss: 0.0002719042\n",
            "Loss: 0.0002714877\n",
            "Loss: 0.00027232\n",
            "Loss: 0.00027136886\n",
            "Loss: 0.00027109607\n",
            "Loss: 0.0002707283\n",
            "Loss: 0.00027045287\n",
            "Loss: 0.0002701437\n",
            "Loss: 0.00026977132\n",
            "Loss: 0.0002693519\n",
            "Loss: 0.0002688269\n",
            "Loss: 0.00026818196\n",
            "Loss: 0.00026752416\n",
            "Loss: 0.0002670691\n",
            "Loss: 0.00026665864\n",
            "Loss: 0.00026625476\n",
            "Loss: 0.00026568127\n",
            "Loss: 0.0002652541\n",
            "Loss: 0.0002649165\n",
            "Loss: 0.0002645337\n",
            "Loss: 0.00026421694\n",
            "Loss: 0.00026369147\n",
            "Loss: 0.00026324272\n",
            "Loss: 0.00026303538\n",
            "Loss: 0.00026260834\n",
            "Loss: 0.00026244082\n",
            "Loss: 0.0002622353\n",
            "Loss: 0.00026205555\n",
            "Loss: 0.00026182015\n",
            "Loss: 0.0002616521\n",
            "Loss: 0.000261462\n",
            "Loss: 0.0002613331\n",
            "Loss: 0.00026105926\n",
            "Loss: 0.00026067867\n",
            "Loss: 0.00026025862\n",
            "Loss: 0.00025976117\n",
            "Loss: 0.00025950058\n",
            "Loss: 0.00025933547\n",
            "Loss: 0.00025905878\n",
            "Loss: 0.00025892607\n",
            "Loss: 0.0002587876\n",
            "Loss: 0.00025847607\n",
            "Loss: 0.0002578299\n",
            "Loss: 0.00025939706\n",
            "Loss: 0.0002576023\n",
            "Loss: 0.00025701473\n",
            "Loss: 0.0002566161\n",
            "Loss: 0.00025619107\n",
            "Loss: 0.00025578303\n",
            "Loss: 0.00025600594\n",
            "Loss: 0.00025554482\n",
            "Loss: 0.00025523815\n",
            "Loss: 0.00025501777\n",
            "Loss: 0.0002547257\n",
            "Loss: 0.00025425354\n",
            "Loss: 0.0002540617\n",
            "Loss: 0.00025366835\n",
            "Loss: 0.00025351567\n",
            "Loss: 0.000253302\n",
            "Loss: 0.00025310987\n",
            "Loss: 0.00025263656\n",
            "Loss: 0.00025219296\n",
            "Loss: 0.0002519089\n",
            "Loss: 0.00025159953\n",
            "Loss: 0.00025105497\n",
            "Loss: 0.00025178466\n",
            "Loss: 0.00025074568\n",
            "Loss: 0.00025030063\n",
            "Loss: 0.0002498093\n",
            "Loss: 0.00024957975\n",
            "Loss: 0.00024927815\n",
            "Loss: 0.0002490872\n",
            "Loss: 0.00024886272\n",
            "Loss: 0.00024855958\n",
            "Loss: 0.0002482659\n",
            "Loss: 0.0002478236\n",
            "Loss: 0.00024726737\n",
            "Loss: 0.00024743678\n",
            "Loss: 0.00024689664\n",
            "Loss: 0.00024640007\n",
            "Loss: 0.0002459955\n",
            "Loss: 0.00024573415\n",
            "Loss: 0.00024546694\n",
            "Loss: 0.00024525\n",
            "Loss: 0.00024490757\n",
            "Loss: 0.0002447359\n",
            "Loss: 0.00024433166\n",
            "Loss: 0.00024394738\n",
            "Loss: 0.00024364144\n",
            "Loss: 0.00024312412\n",
            "Loss: 0.00024286902\n",
            "Loss: 0.00024251091\n",
            "Loss: 0.00024211257\n",
            "Loss: 0.00024190167\n",
            "Loss: 0.00024150017\n",
            "Loss: 0.00024135245\n",
            "Loss: 0.00024091845\n",
            "Loss: 0.00024076959\n",
            "Loss: 0.00024045876\n",
            "Loss: 0.00024026245\n",
            "Loss: 0.00024001382\n",
            "Loss: 0.00023971287\n",
            "Loss: 0.00023940962\n",
            "Loss: 0.00023919708\n",
            "Loss: 0.00023895709\n",
            "Loss: 0.00023871342\n",
            "Loss: 0.00023845225\n",
            "Loss: 0.00023808707\n",
            "Loss: 0.00023794721\n",
            "Loss: 0.00023761175\n",
            "Loss: 0.00023843048\n",
            "Loss: 0.00023749052\n",
            "Loss: 0.00023728271\n",
            "Loss: 0.00023700617\n",
            "Loss: 0.00023679077\n",
            "Loss: 0.0002367146\n",
            "Loss: 0.00023620506\n",
            "Loss: 0.00023604388\n",
            "Loss: 0.00023588302\n",
            "Loss: 0.00023571265\n",
            "Loss: 0.00023519849\n",
            "Loss: 0.00023489626\n",
            "Loss: 0.00023445624\n",
            "Loss: 0.00023405228\n",
            "Loss: 0.00023359788\n",
            "Loss: 0.0002341796\n",
            "Loss: 0.00023332342\n",
            "Loss: 0.00023280617\n",
            "Loss: 0.00023236411\n",
            "Loss: 0.00023190294\n",
            "Loss: 0.00023133987\n",
            "Loss: 0.0002308451\n",
            "Loss: 0.00023057027\n",
            "Loss: 0.00023004337\n",
            "Loss: 0.00022972532\n",
            "Loss: 0.00022936385\n",
            "Loss: 0.00022914243\n",
            "Loss: 0.00022881568\n",
            "Loss: 0.00022859586\n",
            "Loss: 0.0002283748\n",
            "Loss: 0.00022819039\n",
            "Loss: 0.00022774535\n",
            "Loss: 0.00022742289\n",
            "Loss: 0.00022712012\n",
            "Loss: 0.00022672088\n",
            "Loss: 0.00022643823\n",
            "Loss: 0.00022584855\n",
            "Loss: 0.00022553193\n",
            "Loss: 0.00022687245\n",
            "Loss: 0.00022538632\n",
            "Loss: 0.00022496801\n",
            "Loss: 0.00022464353\n",
            "Loss: 0.00022421188\n",
            "Loss: 0.0002238584\n",
            "Loss: 0.00022410302\n",
            "Loss: 0.00022356605\n",
            "Loss: 0.00022299754\n",
            "Loss: 0.00022267029\n",
            "Loss: 0.00022239142\n",
            "Loss: 0.0002221736\n",
            "Loss: 0.00022203199\n",
            "Loss: 0.00022176182\n",
            "Loss: 0.00022154932\n",
            "Loss: 0.00022139953\n",
            "Loss: 0.00022100426\n",
            "Loss: 0.00022065987\n",
            "Loss: 0.0002204081\n",
            "Loss: 0.00021983287\n",
            "Loss: 0.00021968849\n",
            "Loss: 0.00021943127\n",
            "Loss: 0.00021910454\n",
            "Loss: 0.00021916613\n",
            "Loss: 0.00021891604\n",
            "Loss: 0.00021863784\n",
            "Loss: 0.00021844667\n",
            "Loss: 0.00021819584\n",
            "Loss: 0.00021771861\n",
            "Loss: 0.00021703934\n",
            "Loss: 0.00021652921\n",
            "Loss: 0.00021583404\n",
            "Loss: 0.00021548856\n",
            "Loss: 0.0002154721\n",
            "Loss: 0.000215296\n",
            "Loss: 0.00021501555\n",
            "Loss: 0.00021476552\n",
            "Loss: 0.00021443947\n",
            "Loss: 0.0002141317\n",
            "Loss: 0.00021371085\n",
            "Loss: 0.00021296399\n",
            "Loss: 0.00021255143\n",
            "Loss: 0.00021195802\n",
            "Loss: 0.00021174349\n",
            "Loss: 0.00021142213\n",
            "Loss: 0.00021120053\n",
            "Loss: 0.00021101485\n",
            "Loss: 0.0002110301\n",
            "Loss: 0.00021090909\n",
            "Loss: 0.00021072074\n",
            "Loss: 0.00021056394\n",
            "Loss: 0.00021037625\n",
            "Loss: 0.00021012625\n",
            "Loss: 0.00021044016\n",
            "Loss: 0.00020996887\n",
            "Loss: 0.00020962508\n",
            "Loss: 0.00020939307\n",
            "Loss: 0.00020913844\n",
            "Loss: 0.00020940934\n",
            "Loss: 0.00020904193\n",
            "Loss: 0.00020887691\n",
            "Loss: 0.00020862935\n",
            "Loss: 0.0002083929\n",
            "Loss: 0.00020808015\n",
            "Loss: 0.00020787804\n",
            "Loss: 0.00020745766\n",
            "Loss: 0.00020728898\n",
            "Loss: 0.00020713931\n",
            "Loss: 0.00020698714\n",
            "Loss: 0.00020675112\n",
            "Loss: 0.00020653119\n",
            "Loss: 0.00020625183\n",
            "Loss: 0.0002060468\n",
            "Loss: 0.00020577399\n",
            "Loss: 0.00020558546\n",
            "Loss: 0.00020540389\n",
            "Loss: 0.00020523036\n",
            "Loss: 0.00020507086\n",
            "Loss: 0.00020489623\n",
            "Loss: 0.00020475508\n",
            "Loss: 0.00020452973\n",
            "Loss: 0.00020432957\n",
            "Loss: 0.00020418459\n",
            "Loss: 0.00020396028\n",
            "Loss: 0.00020373703\n",
            "Loss: 0.00020304328\n",
            "Loss: 0.0002025502\n",
            "Loss: 0.0002021715\n",
            "Loss: 0.00020183857\n",
            "Loss: 0.00020162732\n",
            "Loss: 0.00020143027\n",
            "Loss: 0.00020130239\n",
            "Loss: 0.0002011605\n",
            "Loss: 0.00020087187\n",
            "Loss: 0.00020066879\n",
            "Loss: 0.00020034544\n",
            "Loss: 0.00020033811\n",
            "Loss: 0.00020017868\n",
            "Loss: 0.00019995985\n",
            "Loss: 0.00019970862\n",
            "Loss: 0.00019951766\n",
            "Loss: 0.0001992617\n",
            "Loss: 0.00019902625\n",
            "Loss: 0.00019891387\n",
            "Loss: 0.00019884735\n",
            "Loss: 0.00019879577\n",
            "Loss: 0.00019855378\n",
            "Loss: 0.0001983783\n",
            "Loss: 0.00019824953\n",
            "Loss: 0.00019794183\n",
            "Loss: 0.000197816\n",
            "Loss: 0.00019768065\n",
            "Loss: 0.00019754517\n",
            "Loss: 0.00019738762\n",
            "Loss: 0.00019729632\n",
            "Loss: 0.00019711368\n",
            "Loss: 0.0001968877\n",
            "Loss: 0.00019664686\n",
            "Loss: 0.00019628846\n",
            "Loss: 0.00019607236\n",
            "Loss: 0.0001958175\n",
            "Loss: 0.00019569881\n",
            "Loss: 0.00019553339\n",
            "Loss: 0.00019521792\n",
            "Loss: 0.00019515384\n",
            "Loss: 0.00019491147\n",
            "Loss: 0.00019472453\n",
            "Loss: 0.00019456394\n",
            "Loss: 0.0001943795\n",
            "Loss: 0.00019420093\n",
            "Loss: 0.00019402232\n",
            "Loss: 0.00019385823\n",
            "Loss: 0.00019367131\n",
            "Loss: 0.00019353634\n",
            "Loss: 0.00019336799\n",
            "Loss: 0.00019322307\n",
            "Loss: 0.00019289434\n",
            "Loss: 0.00019264172\n",
            "Loss: 0.00019228159\n",
            "Loss: 0.00019233025\n",
            "Loss: 0.00019206619\n",
            "Loss: 0.00019164447\n",
            "Loss: 0.00019123298\n",
            "Loss: 0.0001908098\n",
            "Loss: 0.00019059374\n",
            "Loss: 0.00019034182\n",
            "Loss: 0.00019010698\n",
            "Loss: 0.00018982429\n",
            "Loss: 0.00018980523\n",
            "Loss: 0.00018965069\n",
            "Loss: 0.00018930364\n",
            "Loss: 0.00018910547\n",
            "Loss: 0.0001888474\n",
            "Loss: 0.00018864794\n",
            "Loss: 0.00018848147\n",
            "Loss: 0.00018819621\n",
            "Loss: 0.00018794712\n",
            "Loss: 0.00018775699\n",
            "Loss: 0.00018745064\n",
            "Loss: 0.0001873255\n",
            "Loss: 0.00018692177\n",
            "Loss: 0.00018669137\n",
            "Loss: 0.00018651814\n",
            "Loss: 0.00018657529\n",
            "Loss: 0.00018627872\n",
            "Loss: 0.00018597796\n",
            "Loss: 0.00018581914\n",
            "Loss: 0.00018563983\n",
            "Loss: 0.00018551735\n",
            "Loss: 0.00018534773\n",
            "Loss: 0.00018525723\n",
            "Loss: 0.00018514995\n",
            "Loss: 0.00018496272\n",
            "Loss: 0.00018464604\n",
            "Loss: 0.00018442795\n",
            "Loss: 0.0001842264\n",
            "Loss: 0.00018402086\n",
            "Loss: 0.00018383138\n",
            "Loss: 0.00018358385\n",
            "Loss: 0.00018330352\n",
            "Loss: 0.00018501999\n",
            "Loss: 0.00018323064\n",
            "Loss: 0.00018309357\n",
            "Loss: 0.00018286827\n",
            "Loss: 0.00018258968\n",
            "Loss: 0.00018252667\n",
            "Loss: 0.00018227377\n",
            "Loss: 0.00018214615\n",
            "Loss: 0.00018200204\n",
            "Loss: 0.0001818637\n",
            "Loss: 0.00018177429\n",
            "Loss: 0.0001815397\n",
            "Loss: 0.00018138773\n",
            "Loss: 0.00018113044\n",
            "Loss: 0.00018076971\n",
            "Loss: 0.00018034957\n",
            "Loss: 0.00018006924\n",
            "Loss: 0.00017987263\n",
            "Loss: 0.0001796367\n",
            "Loss: 0.00017947271\n",
            "Loss: 0.0001792811\n",
            "Loss: 0.00017917104\n",
            "Loss: 0.00017892709\n",
            "Loss: 0.00017863822\n",
            "Loss: 0.00017877862\n",
            "Loss: 0.00017842028\n",
            "Loss: 0.00017820152\n",
            "Loss: 0.00017795831\n",
            "Loss: 0.00017781479\n",
            "Loss: 0.00017763427\n",
            "Loss: 0.00017740822\n",
            "Loss: 0.00017713616\n",
            "Loss: 0.0001770122\n",
            "Loss: 0.00017731324\n",
            "Loss: 0.0001769068\n",
            "Loss: 0.00017674148\n",
            "Loss: 0.00017659433\n",
            "Loss: 0.00017633499\n",
            "Loss: 0.00017614565\n",
            "Loss: 0.00017582\n",
            "Loss: 0.00017561298\n",
            "Loss: 0.00017528038\n",
            "Loss: 0.00017510666\n",
            "Loss: 0.00017497948\n",
            "Loss: 0.00017484509\n",
            "Loss: 0.00017471981\n",
            "Loss: 0.00017454676\n",
            "Loss: 0.00017426579\n",
            "Loss: 0.00017416858\n",
            "Loss: 0.00017393776\n",
            "Loss: 0.00017385138\n",
            "Loss: 0.00017368095\n",
            "Loss: 0.00017348601\n",
            "Loss: 0.00017323924\n",
            "Loss: 0.00017302386\n",
            "Loss: 0.0001729907\n",
            "Loss: 0.0001727367\n",
            "Loss: 0.0001726051\n",
            "Loss: 0.0001724064\n",
            "Loss: 0.0001722493\n",
            "Loss: 0.00017219354\n",
            "Loss: 0.00017196522\n",
            "Loss: 0.00017186376\n",
            "Loss: 0.00017168114\n",
            "Loss: 0.00017151043\n",
            "Loss: 0.00017232569\n",
            "Loss: 0.00017141938\n",
            "Loss: 0.00017119784\n",
            "Loss: 0.00017102584\n",
            "Loss: 0.00017067495\n",
            "Loss: 0.00017038712\n",
            "Loss: 0.00017007945\n",
            "Loss: 0.00017068918\n",
            "Loss: 0.00016994163\n",
            "Loss: 0.00016980726\n",
            "Loss: 0.00016962184\n",
            "Loss: 0.0001694673\n",
            "Loss: 0.00016921027\n",
            "Loss: 0.0001693491\n",
            "Loss: 0.00016908822\n",
            "Loss: 0.00016892429\n",
            "Loss: 0.00016877893\n",
            "Loss: 0.00016866028\n",
            "Loss: 0.00016832387\n",
            "Loss: 0.00016801775\n",
            "Loss: 0.00016793772\n",
            "Loss: 0.00016744783\n",
            "Loss: 0.00016732898\n",
            "Loss: 0.00016721513\n",
            "Loss: 0.00016718215\n",
            "Loss: 0.00016709707\n",
            "Loss: 0.00016703903\n",
            "Loss: 0.00016698513\n",
            "Loss: 0.00016690104\n",
            "Loss: 0.00016677211\n",
            "Loss: 0.0001672478\n",
            "Loss: 0.00016671434\n",
            "Loss: 0.00016661917\n",
            "Loss: 0.00016651367\n",
            "Loss: 0.00016639016\n",
            "Loss: 0.00016628744\n",
            "Loss: 0.00016611264\n",
            "Loss: 0.00016597958\n",
            "Loss: 0.00016584946\n",
            "Loss: 0.00016572006\n",
            "Loss: 0.00016556063\n",
            "Loss: 0.00016539035\n",
            "Loss: 0.00016510986\n",
            "Loss: 0.00016518758\n",
            "Loss: 0.0001649816\n",
            "Loss: 0.00016472329\n",
            "Loss: 0.00016461598\n",
            "Loss: 0.00016450402\n",
            "Loss: 0.0001643867\n",
            "Loss: 0.00016463753\n",
            "Loss: 0.00016429246\n",
            "Loss: 0.00016405687\n",
            "Loss: 0.00016391938\n",
            "Loss: 0.00016375107\n",
            "Loss: 0.00016362191\n",
            "Loss: 0.00016348273\n",
            "Loss: 0.00016327384\n",
            "Loss: 0.00016316568\n",
            "Loss: 0.00016301041\n",
            "Loss: 0.00016288538\n",
            "Loss: 0.00016269999\n",
            "Loss: 0.00016246893\n",
            "Loss: 0.0001623131\n",
            "Loss: 0.00016225135\n",
            "Loss: 0.00016218671\n",
            "Loss: 0.00016242919\n",
            "Loss: 0.00016213849\n",
            "Loss: 0.00016204915\n",
            "Loss: 0.0001619058\n",
            "Loss: 0.00016183191\n",
            "Loss: 0.00016173135\n",
            "Loss: 0.00016158687\n",
            "Loss: 0.00016153895\n",
            "Loss: 0.00016132921\n",
            "Loss: 0.00016123283\n",
            "Loss: 0.00016109386\n",
            "Loss: 0.00016093242\n",
            "Loss: 0.00016094245\n",
            "Loss: 0.0001608611\n",
            "Loss: 0.00016076952\n",
            "Loss: 0.00016071269\n",
            "Loss: 0.00016061132\n",
            "Loss: 0.00016051473\n",
            "Loss: 0.00016039594\n",
            "Loss: 0.00016029918\n",
            "Loss: 0.00016018774\n",
            "Loss: 0.00016009531\n",
            "Loss: 0.000160236\n",
            "Loss: 0.00016005097\n",
            "Loss: 0.00015995541\n",
            "Loss: 0.00015988311\n",
            "Loss: 0.00015980742\n",
            "Loss: 0.00015971062\n",
            "Loss: 0.00015962863\n",
            "Loss: 0.00015953054\n",
            "Loss: 0.0001594846\n",
            "Loss: 0.00015942252\n",
            "Loss: 0.00015936849\n",
            "Loss: 0.0001592278\n",
            "Loss: 0.00015904047\n",
            "Loss: 0.00015883776\n",
            "Loss: 0.00015881626\n",
            "Loss: 0.00015869853\n",
            "Loss: 0.00015860188\n",
            "Loss: 0.0001584347\n",
            "Loss: 0.00015908899\n",
            "Loss: 0.0001583895\n",
            "Loss: 0.00015824736\n",
            "Loss: 0.00015805347\n",
            "Loss: 0.00015796404\n",
            "Loss: 0.0001578644\n",
            "Loss: 0.00015781332\n",
            "Loss: 0.00015775961\n",
            "Loss: 0.00015769486\n",
            "Loss: 0.00015759707\n",
            "Loss: 0.00015747774\n",
            "Loss: 0.00015733961\n",
            "Loss: 0.00015722463\n",
            "Loss: 0.00015711266\n",
            "Loss: 0.00015702216\n",
            "Loss: 0.0001568796\n",
            "Loss: 0.00015669077\n",
            "Loss: 0.0001564805\n",
            "Loss: 0.0001563495\n",
            "Loss: 0.00015623236\n",
            "Loss: 0.00015614869\n",
            "Loss: 0.0001560048\n",
            "Loss: 0.00015581408\n",
            "Loss: 0.00015575235\n",
            "Loss: 0.00015562045\n",
            "Loss: 0.00015537132\n",
            "Loss: 0.00015519981\n",
            "Loss: 0.00015486463\n",
            "Loss: 0.00015474187\n",
            "Loss: 0.00015492656\n",
            "Loss: 0.00015469873\n",
            "Loss: 0.00015461887\n",
            "Loss: 0.00015448514\n",
            "Loss: 0.00015428981\n",
            "Loss: 0.00015425545\n",
            "Loss: 0.00015404176\n",
            "Loss: 0.00015395199\n",
            "Loss: 0.0001538566\n",
            "Loss: 0.00015375094\n",
            "Loss: 0.00015370084\n",
            "Loss: 0.00015354769\n",
            "Loss: 0.00015348167\n",
            "Loss: 0.00015332589\n",
            "Loss: 0.00015321634\n",
            "Loss: 0.00015307617\n",
            "Loss: 0.00015297504\n",
            "Loss: 0.00015286109\n",
            "Loss: 0.00015273158\n",
            "Loss: 0.00015247462\n",
            "Loss: 0.00015215154\n",
            "Loss: 0.00015237046\n",
            "Loss: 0.00015200165\n",
            "Loss: 0.00015181224\n",
            "Loss: 0.00015168585\n",
            "Loss: 0.00015152988\n",
            "Loss: 0.0001515212\n",
            "Loss: 0.00015134402\n",
            "Loss: 0.00015109166\n",
            "Loss: 0.00015085895\n",
            "Loss: 0.00015066286\n",
            "Loss: 0.00015052647\n",
            "Loss: 0.00015040545\n",
            "Loss: 0.00015030753\n",
            "Loss: 0.00015023246\n",
            "Loss: 0.0001501774\n",
            "Loss: 0.00014998706\n",
            "Loss: 0.00014988173\n",
            "Loss: 0.00014970225\n",
            "Loss: 0.00014953414\n",
            "Loss: 0.00014939078\n",
            "Loss: 0.00014924264\n",
            "Loss: 0.00014914371\n",
            "Loss: 0.000148974\n",
            "Loss: 0.0001488155\n",
            "Loss: 0.00014869904\n",
            "Loss: 0.00014848664\n",
            "Loss: 0.00014831735\n",
            "Loss: 0.00014815695\n",
            "Loss: 0.00014788454\n",
            "Loss: 0.00014776952\n",
            "Loss: 0.00014756831\n",
            "Loss: 0.00014810862\n",
            "Loss: 0.00014750959\n",
            "Loss: 0.00014740124\n",
            "Loss: 0.00014728241\n",
            "Loss: 0.00014726388\n",
            "Loss: 0.00014717062\n",
            "Loss: 0.00014709032\n",
            "Loss: 0.00014698147\n",
            "Loss: 0.00014686352\n",
            "Loss: 0.00014676356\n",
            "Loss: 0.0001466888\n",
            "Loss: 0.00014661865\n",
            "Loss: 0.00014650979\n",
            "Loss: 0.0001463011\n",
            "Loss: 0.00014636994\n",
            "Loss: 0.00014618564\n",
            "Loss: 0.000146053\n",
            "Loss: 0.0001459426\n",
            "Loss: 0.00014582512\n",
            "Loss: 0.00014581438\n",
            "Loss: 0.00014574167\n",
            "Loss: 0.00014564217\n",
            "Loss: 0.00014553312\n",
            "Loss: 0.00014543341\n",
            "Loss: 0.0001452383\n",
            "Loss: 0.0001459519\n",
            "Loss: 0.00014515378\n",
            "Loss: 0.0001449751\n",
            "Loss: 0.0001449109\n",
            "Loss: 0.0001447623\n",
            "Loss: 0.00014466175\n",
            "Loss: 0.00014453041\n",
            "Loss: 0.00014429339\n",
            "Loss: 0.00014419676\n",
            "Loss: 0.00014382054\n",
            "Loss: 0.00014367883\n",
            "Loss: 0.00014343177\n",
            "Loss: 0.00014337956\n",
            "Loss: 0.00014317952\n",
            "Loss: 0.00014306269\n",
            "Loss: 0.00014289404\n",
            "Loss: 0.0001428306\n",
            "Loss: 0.00014263544\n",
            "Loss: 0.00014252118\n",
            "Loss: 0.00014239947\n",
            "Loss: 0.00014218438\n",
            "Loss: 0.0001418133\n",
            "Loss: 0.00014183123\n",
            "Loss: 0.0001416127\n",
            "Loss: 0.00014138134\n",
            "Loss: 0.00014123612\n",
            "Loss: 0.00014114464\n",
            "Loss: 0.00014104537\n",
            "Loss: 0.0001409461\n",
            "Loss: 0.00014078099\n",
            "Loss: 0.00014067286\n",
            "Loss: 0.0001403213\n",
            "Loss: 0.00013996301\n",
            "Loss: 0.00014022819\n",
            "Loss: 0.00013978605\n",
            "Loss: 0.00013962621\n",
            "Loss: 0.00013951471\n",
            "Loss: 0.00013937635\n",
            "Loss: 0.00013911352\n",
            "Loss: 0.00013930038\n",
            "Loss: 0.00013899291\n",
            "Loss: 0.00013881628\n",
            "Loss: 0.00013869646\n",
            "Loss: 0.00013861136\n",
            "Loss: 0.00013848045\n",
            "Loss: 0.00013826357\n",
            "Loss: 0.00013803988\n",
            "Loss: 0.00013789226\n",
            "Loss: 0.00013781244\n",
            "Loss: 0.00013774652\n",
            "Loss: 0.00013761067\n",
            "Loss: 0.00013746132\n",
            "Loss: 0.00013737901\n",
            "Loss: 0.00013710701\n",
            "Loss: 0.0001370069\n",
            "Loss: 0.00013690113\n",
            "Loss: 0.00013676686\n",
            "Loss: 0.00013654398\n",
            "Loss: 0.00013641987\n",
            "Loss: 0.00013632677\n",
            "Loss: 0.00013624725\n",
            "Loss: 0.00013615242\n",
            "Loss: 0.00013587161\n",
            "Loss: 0.00013549306\n",
            "Loss: 0.00013564283\n",
            "Loss: 0.00013532862\n",
            "Loss: 0.00013507986\n",
            "Loss: 0.00013496698\n",
            "Loss: 0.00013485068\n",
            "Loss: 0.00013465232\n",
            "Loss: 0.0001343915\n",
            "Loss: 0.00013414035\n",
            "Loss: 0.00013397391\n",
            "Loss: 0.00013380416\n",
            "Loss: 0.00013363623\n",
            "Loss: 0.00013336787\n",
            "Loss: 0.00013313306\n",
            "Loss: 0.00013291932\n",
            "Loss: 0.00013280625\n",
            "Loss: 0.00013265645\n",
            "Loss: 0.00013242678\n",
            "Loss: 0.00013227412\n",
            "Loss: 0.00013200977\n",
            "Loss: 0.00013172421\n",
            "Loss: 0.00013159993\n",
            "Loss: 0.0001313384\n",
            "Loss: 0.00013113767\n",
            "Loss: 0.00013101033\n",
            "Loss: 0.00013094944\n",
            "Loss: 0.00013083097\n",
            "Loss: 0.00013077422\n",
            "Loss: 0.00013066338\n",
            "Loss: 0.00013047794\n",
            "Loss: 0.00013258801\n",
            "Loss: 0.00013043018\n",
            "Loss: 0.00013021886\n",
            "Loss: 0.00013008666\n",
            "Loss: 0.00012995384\n",
            "Loss: 0.00012981196\n",
            "Loss: 0.0001298199\n",
            "Loss: 0.00012972449\n",
            "Loss: 0.00012960227\n",
            "Loss: 0.0001295264\n",
            "Loss: 0.0001294602\n",
            "Loss: 0.00012931516\n",
            "Loss: 0.0001291496\n",
            "Loss: 0.00012904726\n",
            "Loss: 0.00012892319\n",
            "Loss: 0.00013001778\n",
            "Loss: 0.0001288838\n",
            "Loss: 0.00012874714\n",
            "Loss: 0.00012855587\n",
            "Loss: 0.00012837039\n",
            "Loss: 0.00012823282\n",
            "Loss: 0.00012848394\n",
            "Loss: 0.00012814382\n",
            "Loss: 0.00012800671\n",
            "Loss: 0.00012782533\n",
            "Loss: 0.0001276791\n",
            "Loss: 0.00012743365\n",
            "Loss: 0.00012769706\n",
            "Loss: 0.0001273121\n",
            "Loss: 0.00012703445\n",
            "Loss: 0.00012688653\n",
            "Loss: 0.00012669078\n",
            "Loss: 0.0001265156\n",
            "Loss: 0.00012637215\n",
            "Loss: 0.00012615699\n",
            "Loss: 0.0001260149\n",
            "Loss: 0.00012591184\n",
            "Loss: 0.00012568719\n",
            "Loss: 0.00012554368\n",
            "Loss: 0.00012540694\n",
            "Loss: 0.0001252141\n",
            "Loss: 0.00012515603\n",
            "Loss: 0.00012505522\n",
            "Loss: 0.00012491149\n",
            "Loss: 0.00012537166\n",
            "Loss: 0.0001248562\n",
            "Loss: 0.00012471226\n",
            "Loss: 0.00012459821\n",
            "Loss: 0.00012446026\n",
            "Loss: 0.00012431419\n",
            "Loss: 0.00012415378\n",
            "Loss: 0.00012396615\n",
            "Loss: 0.00012386212\n",
            "Loss: 0.00012372705\n",
            "Loss: 0.00012362632\n",
            "Loss: 0.00012349716\n",
            "Loss: 0.00012343128\n",
            "Loss: 0.00012333153\n",
            "Loss: 0.00012322792\n",
            "Loss: 0.00012308714\n",
            "Loss: 0.00012293425\n",
            "Loss: 0.00012287789\n",
            "Loss: 0.00012273864\n",
            "Loss: 0.00012275061\n",
            "Loss: 0.00012266486\n",
            "Loss: 0.00012254895\n",
            "Loss: 0.0001224778\n",
            "Loss: 0.00012241484\n",
            "Loss: 0.0001223576\n",
            "Loss: 0.00012226106\n",
            "Loss: 0.00012218248\n",
            "Loss: 0.00012213444\n",
            "Loss: 0.00012207174\n",
            "Loss: 0.0001219683\n",
            "Loss: 0.00012185096\n",
            "Loss: 0.000121692254\n",
            "Loss: 0.0001215947\n",
            "Loss: 0.00012141114\n",
            "Loss: 0.00012140941\n",
            "Loss: 0.000121322046\n",
            "Loss: 0.00012118168\n",
            "Loss: 0.0001210635\n",
            "Loss: 0.00012090444\n",
            "Loss: 0.000120666045\n",
            "Loss: 0.000120341705\n",
            "Loss: 0.00012037593\n",
            "Loss: 0.00012016283\n",
            "Loss: 0.00012002133\n",
            "Loss: 0.00011984403\n",
            "Loss: 0.00011972479\n",
            "Loss: 0.000119614604\n",
            "Loss: 0.00011953227\n",
            "Loss: 0.00011936298\n",
            "Loss: 0.000119147066\n",
            "Loss: 0.00011905185\n",
            "Loss: 0.00011885256\n",
            "Loss: 0.000118776254\n",
            "Loss: 0.00011861192\n",
            "Loss: 0.000118405005\n",
            "Loss: 0.00011823706\n",
            "Loss: 0.000118335454\n",
            "Loss: 0.00011814441\n",
            "Loss: 0.00011804965\n",
            "Loss: 0.00011793703\n",
            "Loss: 0.000117834395\n",
            "Loss: 0.0001177488\n",
            "Loss: 0.000117676915\n",
            "Loss: 0.000117584816\n",
            "Loss: 0.000117497504\n",
            "Loss: 0.000117246294\n",
            "Loss: 0.000117202544\n",
            "Loss: 0.000116859446\n",
            "Loss: 0.00011674094\n",
            "Loss: 0.000116581796\n",
            "Loss: 0.00011650751\n",
            "Loss: 0.000116312716\n",
            "Loss: 0.000116216615\n",
            "Loss: 0.00011604175\n",
            "Loss: 0.00011577811\n",
            "Loss: 0.00011550717\n",
            "Loss: 0.000115700124\n",
            "Loss: 0.00011540147\n",
            "Loss: 0.00011528254\n",
            "Loss: 0.00011523609\n",
            "Loss: 0.00011516969\n",
            "Loss: 0.00011512861\n",
            "Loss: 0.00011507547\n",
            "Loss: 0.000115014205\n",
            "Loss: 0.0001149635\n",
            "Loss: 0.000114835624\n",
            "Loss: 0.00011517298\n",
            "Loss: 0.000114794675\n",
            "Loss: 0.00011469137\n",
            "Loss: 0.00011461482\n",
            "Loss: 0.0001145371\n",
            "Loss: 0.000114423994\n",
            "Loss: 0.0001142455\n",
            "Loss: 0.00011414851\n",
            "Loss: 0.000113934795\n",
            "Loss: 0.00011383745\n",
            "Loss: 0.00011373733\n",
            "Loss: 0.00011376015\n",
            "Loss: 0.00011366431\n",
            "Loss: 0.00011355075\n",
            "Loss: 0.000113451824\n",
            "Loss: 0.00011338335\n",
            "Loss: 0.00011329926\n",
            "Loss: 0.0001132395\n",
            "Loss: 0.000113152346\n",
            "Loss: 0.000113045564\n",
            "Loss: 0.00011317778\n",
            "Loss: 0.000112992755\n",
            "Loss: 0.00011288388\n",
            "Loss: 0.000112819784\n",
            "Loss: 0.00011266254\n",
            "Loss: 0.00011254367\n",
            "Loss: 0.000112537055\n",
            "Loss: 0.00011243284\n",
            "Loss: 0.000112259746\n",
            "Loss: 0.00011208764\n",
            "Loss: 0.000111931644\n",
            "Loss: 0.00011175581\n",
            "Loss: 0.000112678055\n",
            "Loss: 0.000111693415\n",
            "Loss: 0.00011162235\n",
            "Loss: 0.000111479414\n",
            "Loss: 0.00011135092\n",
            "Loss: 0.000111192625\n",
            "Loss: 0.00011103359\n",
            "Loss: 0.000111058485\n",
            "Loss: 0.00011096835\n",
            "Loss: 0.000110853056\n",
            "Loss: 0.000110769135\n",
            "Loss: 0.000110757974\n",
            "Loss: 0.00011066421\n",
            "Loss: 0.00011062717\n",
            "Loss: 0.000110530564\n",
            "Loss: 0.0001104182\n",
            "Loss: 0.000110168374\n",
            "Loss: 0.0001102596\n",
            "Loss: 0.00011004592\n",
            "Loss: 0.00010994541\n",
            "Loss: 0.00010989264\n",
            "Loss: 0.000109851\n",
            "Loss: 0.00010976211\n",
            "Loss: 0.00010971384\n",
            "Loss: 0.000109624336\n",
            "Loss: 0.00010957148\n",
            "Loss: 0.00010952748\n",
            "Loss: 0.00010948168\n",
            "Loss: 0.0001093788\n",
            "Loss: 0.00010928422\n",
            "Loss: 0.00010917956\n",
            "Loss: 0.0001090469\n",
            "Loss: 0.0001087383\n",
            "Loss: 0.000109736546\n",
            "Loss: 0.00010861349\n",
            "Loss: 0.000108433895\n",
            "Loss: 0.00010832249\n",
            "Loss: 0.00010824808\n",
            "Loss: 0.00010810705\n",
            "Loss: 0.000108008826\n",
            "Loss: 0.00010792533\n",
            "Loss: 0.00010784721\n",
            "Loss: 0.00010778684\n",
            "Loss: 0.000107672226\n",
            "Loss: 0.00010753285\n",
            "Loss: 0.00010738103\n",
            "Loss: 0.00010727766\n",
            "Loss: 0.00010717346\n",
            "Loss: 0.000107083004\n",
            "Loss: 0.00010698981\n",
            "Loss: 0.000106864405\n",
            "Loss: 0.00010669836\n",
            "Loss: 0.00010660754\n",
            "Loss: 0.00010647431\n",
            "Loss: 0.000106376625\n",
            "Loss: 0.00010630517\n",
            "Loss: 0.00010621708\n",
            "Loss: 0.00010615925\n",
            "Loss: 0.00010606306\n",
            "Loss: 0.00010597108\n",
            "Loss: 0.000105739964\n",
            "Loss: 0.00010559121\n",
            "Loss: 0.00010542985\n",
            "Loss: 0.00010531054\n",
            "Loss: 0.00010517916\n",
            "Loss: 0.00010504745\n",
            "Loss: 0.00010497733\n",
            "Loss: 0.00010487748\n",
            "Loss: 0.00010484307\n",
            "Loss: 0.000104802406\n",
            "Loss: 0.00010473572\n",
            "Loss: 0.00010481909\n",
            "Loss: 0.00010468587\n",
            "Loss: 0.00010459219\n",
            "Loss: 0.0001045237\n",
            "Loss: 0.0001044543\n",
            "Loss: 0.00010436859\n",
            "Loss: 0.000104341634\n",
            "Loss: 0.000104193525\n",
            "Loss: 0.0001041524\n",
            "Loss: 0.00010409107\n",
            "Loss: 0.00010422894\n",
            "Loss: 0.000104065664\n",
            "Loss: 0.000104036626\n",
            "Loss: 0.00010395875\n",
            "Loss: 0.00010387314\n",
            "Loss: 0.00010380494\n",
            "Loss: 0.000103746104\n",
            "Loss: 0.000103658196\n",
            "Loss: 0.00010365572\n",
            "Loss: 0.00010361406\n",
            "Loss: 0.00010355834\n",
            "Loss: 0.00010346292\n",
            "Loss: 0.00010342729\n",
            "Loss: 0.0001033297\n",
            "Loss: 0.00010336957\n",
            "Loss: 0.00010328679\n",
            "Loss: 0.00010322087\n",
            "Loss: 0.00010312378\n",
            "Loss: 0.00010305755\n",
            "Loss: 0.00010300265\n",
            "Loss: 0.00010295912\n",
            "Loss: 0.00010289997\n",
            "Loss: 0.000102827325\n",
            "Loss: 0.0001027362\n",
            "Loss: 0.00010255647\n",
            "Loss: 0.00010245131\n",
            "Loss: 0.0001023611\n",
            "Loss: 0.00010229726\n",
            "Loss: 0.00010222687\n",
            "Loss: 0.00010210462\n",
            "Loss: 0.00010197594\n",
            "Loss: 0.0001018817\n",
            "Loss: 0.000101785554\n",
            "Loss: 0.00010169536\n",
            "Loss: 0.00010164225\n",
            "Loss: 0.000101575264\n",
            "Loss: 0.000101503116\n",
            "Loss: 0.00010141803\n",
            "Loss: 0.00010133172\n",
            "Loss: 0.000101321806\n",
            "Loss: 0.00010126549\n",
            "Loss: 0.0001012066\n",
            "Loss: 0.000101107136\n",
            "Loss: 0.00010127839\n",
            "Loss: 0.00010108928\n",
            "Loss: 0.00010105339\n",
            "Loss: 0.000100984595\n",
            "Loss: 0.00010091729\n",
            "Loss: 0.00010081068\n",
            "Loss: 0.00010076346\n",
            "Loss: 0.000100614474\n",
            "Loss: 0.00010055576\n",
            "Loss: 0.000100499106\n",
            "Loss: 0.00010046273\n",
            "Loss: 0.00010039343\n",
            "Loss: 0.00010032379\n",
            "Loss: 0.000100232995\n",
            "Loss: 0.00010034403\n",
            "Loss: 0.00010018848\n",
            "Loss: 0.000100087396\n",
            "Loss: 0.00010001429\n",
            "Loss: 9.992147e-05\n",
            "Loss: 9.979661e-05\n",
            "Loss: 9.974596e-05\n",
            "Loss: 9.9630284e-05\n",
            "Loss: 9.959482e-05\n",
            "Loss: 9.9547215e-05\n",
            "Loss: 9.956915e-05\n",
            "Loss: 9.951893e-05\n",
            "Loss: 9.946669e-05\n",
            "Loss: 9.9425684e-05\n",
            "Loss: 9.939991e-05\n",
            "Loss: 9.934449e-05\n",
            "Loss: 9.929885e-05\n",
            "Loss: 9.920103e-05\n",
            "Loss: 9.9124474e-05\n",
            "Loss: 9.896312e-05\n",
            "Loss: 9.887232e-05\n",
            "Loss: 9.87464e-05\n",
            "Loss: 9.8695906e-05\n",
            "Loss: 9.867993e-05\n",
            "Loss: 9.860177e-05\n",
            "Loss: 9.855222e-05\n",
            "Loss: 9.84878e-05\n",
            "Loss: 9.843704e-05\n",
            "Loss: 9.836332e-05\n",
            "Loss: 9.830625e-05\n",
            "Loss: 9.8246936e-05\n",
            "Loss: 9.818225e-05\n",
            "Loss: 9.815167e-05\n",
            "Loss: 9.8072975e-05\n",
            "Loss: 9.804731e-05\n",
            "Loss: 9.7992364e-05\n",
            "Loss: 9.79044e-05\n",
            "Loss: 9.7820266e-05\n",
            "Loss: 9.76984e-05\n",
            "Loss: 9.760421e-05\n",
            "Loss: 9.7534765e-05\n",
            "Loss: 9.758287e-05\n",
            "Loss: 9.74984e-05\n",
            "Loss: 9.7438664e-05\n",
            "Loss: 9.741222e-05\n",
            "Loss: 9.7332064e-05\n",
            "Loss: 9.7263975e-05\n",
            "Loss: 9.727458e-05\n",
            "Loss: 9.722288e-05\n",
            "Loss: 9.715911e-05\n",
            "Loss: 9.7125005e-05\n",
            "Loss: 9.7055374e-05\n",
            "Loss: 9.696257e-05\n",
            "Loss: 9.687138e-05\n",
            "Loss: 9.679921e-05\n",
            "Loss: 9.675091e-05\n",
            "Loss: 9.672297e-05\n",
            "Loss: 9.664302e-05\n",
            "Loss: 9.657773e-05\n",
            "Loss: 9.651581e-05\n",
            "Loss: 9.6387914e-05\n",
            "Loss: 9.6312e-05\n",
            "Loss: 9.6147196e-05\n",
            "Loss: 9.610303e-05\n",
            "Loss: 9.595111e-05\n",
            "Loss: 9.588612e-05\n",
            "Loss: 9.579465e-05\n",
            "Loss: 9.613547e-05\n",
            "Loss: 9.57639e-05\n",
            "Loss: 9.57002e-05\n",
            "Loss: 9.560748e-05\n",
            "Loss: 9.546455e-05\n",
            "Loss: 9.543092e-05\n",
            "Loss: 9.5314506e-05\n",
            "Loss: 9.527427e-05\n",
            "Loss: 9.522673e-05\n",
            "Loss: 9.516183e-05\n",
            "Loss: 9.504209e-05\n",
            "Loss: 9.494407e-05\n",
            "Loss: 9.480988e-05\n",
            "Loss: 9.470506e-05\n",
            "Loss: 9.455774e-05\n",
            "Loss: 9.4479285e-05\n",
            "Loss: 9.4397124e-05\n",
            "Loss: 9.433519e-05\n",
            "Loss: 9.425821e-05\n",
            "Loss: 9.4141236e-05\n",
            "Loss: 9.407499e-05\n",
            "Loss: 9.394435e-05\n",
            "Loss: 9.387118e-05\n",
            "Loss: 9.3824696e-05\n",
            "Loss: 9.373261e-05\n",
            "Loss: 9.366221e-05\n",
            "Loss: 9.3525894e-05\n",
            "Loss: 9.347222e-05\n",
            "Loss: 9.340995e-05\n",
            "Loss: 9.341184e-05\n",
            "Loss: 9.338418e-05\n",
            "Loss: 9.333278e-05\n",
            "Loss: 9.324154e-05\n",
            "Loss: 9.3114315e-05\n",
            "Loss: 9.296008e-05\n",
            "Loss: 9.3159666e-05\n",
            "Loss: 9.291344e-05\n",
            "Loss: 9.280219e-05\n",
            "Loss: 9.274656e-05\n",
            "Loss: 9.26487e-05\n",
            "Loss: 9.255434e-05\n",
            "Loss: 9.2556584e-05\n",
            "Loss: 9.248679e-05\n",
            "Loss: 9.241575e-05\n",
            "Loss: 9.2341696e-05\n",
            "Loss: 9.228435e-05\n",
            "Loss: 9.213982e-05\n",
            "Loss: 9.2064845e-05\n",
            "Loss: 9.194971e-05\n",
            "Loss: 9.188704e-05\n",
            "Loss: 9.181032e-05\n",
            "Loss: 9.1670445e-05\n",
            "Loss: 9.191033e-05\n",
            "Loss: 9.163385e-05\n",
            "Loss: 9.15677e-05\n",
            "Loss: 9.151148e-05\n",
            "Loss: 9.138686e-05\n",
            "Loss: 9.125551e-05\n",
            "Loss: 9.1523194e-05\n",
            "Loss: 9.1187525e-05\n",
            "Loss: 9.108559e-05\n",
            "Loss: 9.103332e-05\n",
            "Loss: 9.0996575e-05\n",
            "Loss: 9.0928595e-05\n",
            "Loss: 9.084694e-05\n",
            "Loss: 9.077479e-05\n",
            "Loss: 9.0694055e-05\n",
            "Loss: 9.0645044e-05\n",
            "Loss: 9.061517e-05\n",
            "Loss: 9.0526904e-05\n",
            "Loss: 9.04986e-05\n",
            "Loss: 9.0420595e-05\n",
            "Loss: 9.034985e-05\n",
            "Loss: 9.022092e-05\n",
            "Loss: 9.0244095e-05\n",
            "Loss: 9.01328e-05\n",
            "Loss: 9.001091e-05\n",
            "Loss: 8.9927096e-05\n",
            "Loss: 8.985604e-05\n",
            "Loss: 8.9769295e-05\n",
            "Loss: 8.9742396e-05\n",
            "Loss: 8.959765e-05\n",
            "Loss: 8.953271e-05\n",
            "Loss: 8.947568e-05\n",
            "Loss: 8.9368055e-05\n",
            "Loss: 8.972044e-05\n",
            "Loss: 8.932651e-05\n",
            "Loss: 8.92307e-05\n",
            "Loss: 8.919487e-05\n",
            "Loss: 8.915438e-05\n",
            "Loss: 8.91245e-05\n",
            "Loss: 8.903821e-05\n",
            "Loss: 8.8951645e-05\n",
            "Loss: 8.969961e-05\n",
            "Loss: 8.8933826e-05\n",
            "Loss: 8.8874614e-05\n",
            "Loss: 8.88339e-05\n",
            "Loss: 8.875142e-05\n",
            "Loss: 8.8655805e-05\n",
            "Loss: 8.859877e-05\n",
            "Loss: 8.8470246e-05\n",
            "Loss: 8.841278e-05\n",
            "Loss: 8.83404e-05\n",
            "Loss: 8.827295e-05\n",
            "Loss: 8.817979e-05\n",
            "Loss: 8.8911635e-05\n",
            "Loss: 8.8157634e-05\n",
            "Loss: 8.809932e-05\n",
            "Loss: 8.805551e-05\n",
            "Loss: 8.801086e-05\n",
            "Loss: 8.794989e-05\n",
            "Loss: 8.7839515e-05\n",
            "Loss: 8.7686945e-05\n",
            "Loss: 8.76331e-05\n",
            "Loss: 8.749886e-05\n",
            "Loss: 8.7454755e-05\n",
            "Loss: 8.742072e-05\n",
            "Loss: 8.7378525e-05\n",
            "Loss: 8.732595e-05\n",
            "Loss: 8.7252876e-05\n",
            "Loss: 8.721843e-05\n",
            "Loss: 8.721516e-05\n",
            "Loss: 8.717278e-05\n",
            "Loss: 8.7083106e-05\n",
            "Loss: 8.701332e-05\n",
            "Loss: 8.693033e-05\n",
            "Loss: 8.6851374e-05\n",
            "Loss: 8.690765e-05\n",
            "Loss: 8.679312e-05\n",
            "Loss: 8.666664e-05\n",
            "Loss: 8.6565895e-05\n",
            "Loss: 8.642883e-05\n",
            "Loss: 8.627901e-05\n",
            "Loss: 8.614546e-05\n",
            "Loss: 8.594911e-05\n",
            "Loss: 8.5871274e-05\n",
            "Loss: 8.573962e-05\n",
            "Loss: 8.562018e-05\n",
            "Loss: 8.548923e-05\n",
            "Loss: 8.536633e-05\n",
            "Loss: 8.5277585e-05\n",
            "Loss: 8.520395e-05\n",
            "Loss: 8.512453e-05\n",
            "Loss: 8.508163e-05\n",
            "Loss: 8.503423e-05\n",
            "Loss: 8.499513e-05\n",
            "Loss: 8.496591e-05\n",
            "Loss: 8.493259e-05\n",
            "Loss: 8.490263e-05\n",
            "Loss: 8.485829e-05\n",
            "Loss: 8.485008e-05\n",
            "Loss: 8.4800406e-05\n",
            "Loss: 8.477277e-05\n",
            "Loss: 8.474327e-05\n",
            "Loss: 8.471025e-05\n",
            "Loss: 8.47235e-05\n",
            "Loss: 8.467879e-05\n",
            "Loss: 8.460259e-05\n",
            "Loss: 8.452023e-05\n",
            "Loss: 8.444241e-05\n",
            "Loss: 8.4364845e-05\n",
            "Loss: 8.425449e-05\n",
            "Loss: 8.416765e-05\n",
            "Loss: 8.409815e-05\n",
            "Loss: 8.405665e-05\n",
            "Loss: 8.4018095e-05\n",
            "Loss: 8.3931445e-05\n",
            "Loss: 8.38564e-05\n",
            "Loss: 8.38189e-05\n",
            "Loss: 8.369106e-05\n",
            "Loss: 8.363502e-05\n",
            "Loss: 8.356065e-05\n",
            "Loss: 8.3481355e-05\n",
            "Loss: 8.3662715e-05\n",
            "Loss: 8.344016e-05\n",
            "Loss: 8.3352126e-05\n",
            "Loss: 8.326529e-05\n",
            "Loss: 8.3136874e-05\n",
            "Loss: 8.3035964e-05\n",
            "Loss: 8.2940605e-05\n",
            "Loss: 8.282486e-05\n",
            "Loss: 8.275359e-05\n",
            "Loss: 8.279632e-05\n",
            "Loss: 8.268841e-05\n",
            "Loss: 8.256119e-05\n",
            "Loss: 8.249945e-05\n",
            "Loss: 8.241096e-05\n",
            "Loss: 8.231636e-05\n",
            "Loss: 8.21197e-05\n",
            "Loss: 8.296779e-05\n",
            "Loss: 8.206123e-05\n",
            "Loss: 8.1928345e-05\n",
            "Loss: 8.184429e-05\n",
            "Loss: 8.176624e-05\n",
            "Loss: 8.166444e-05\n",
            "Loss: 8.182302e-05\n",
            "Loss: 8.161369e-05\n",
            "Loss: 8.153099e-05\n",
            "Loss: 8.1484e-05\n",
            "Loss: 8.141351e-05\n",
            "Loss: 8.1302336e-05\n",
            "Loss: 8.154544e-05\n",
            "Loss: 8.1249105e-05\n",
            "Loss: 8.117853e-05\n",
            "Loss: 8.1132675e-05\n",
            "Loss: 8.109081e-05\n",
            "Loss: 8.1010236e-05\n",
            "Loss: 8.102557e-05\n",
            "Loss: 8.0955215e-05\n",
            "Loss: 8.090612e-05\n",
            "Loss: 8.0858605e-05\n",
            "Loss: 8.085386e-05\n",
            "Loss: 8.081843e-05\n",
            "Loss: 8.0783895e-05\n",
            "Loss: 8.0721475e-05\n",
            "Loss: 8.0640864e-05\n",
            "Loss: 8.069627e-05\n",
            "Loss: 8.059026e-05\n",
            "Loss: 8.047583e-05\n",
            "Loss: 8.040972e-05\n",
            "Loss: 8.034593e-05\n",
            "Loss: 8.0282225e-05\n",
            "Loss: 8.021481e-05\n",
            "Loss: 8.0165824e-05\n",
            "Loss: 8.01141e-05\n",
            "Loss: 8.005998e-05\n",
            "Loss: 7.9965685e-05\n",
            "Loss: 7.988376e-05\n",
            "Loss: 7.979254e-05\n",
            "Loss: 7.973619e-05\n",
            "Loss: 7.966129e-05\n",
            "Loss: 7.95878e-05\n",
            "Loss: 7.9608144e-05\n",
            "Loss: 7.9534046e-05\n",
            "Loss: 7.9462e-05\n",
            "Loss: 7.942332e-05\n",
            "Loss: 7.936488e-05\n",
            "Loss: 7.928554e-05\n",
            "Loss: 7.921552e-05\n",
            "Loss: 7.916025e-05\n",
            "Loss: 7.9111036e-05\n",
            "Loss: 7.9053156e-05\n",
            "Loss: 7.898889e-05\n",
            "Loss: 7.8954894e-05\n",
            "Loss: 7.888288e-05\n",
            "Loss: 7.877185e-05\n",
            "Loss: 7.864954e-05\n",
            "Loss: 7.856566e-05\n",
            "Loss: 7.852024e-05\n",
            "Loss: 7.847863e-05\n",
            "Loss: 7.843386e-05\n",
            "Loss: 7.833153e-05\n",
            "Loss: 7.829612e-05\n",
            "Loss: 7.821837e-05\n",
            "Loss: 7.817857e-05\n",
            "Loss: 7.813981e-05\n",
            "Loss: 7.8099256e-05\n",
            "Loss: 7.8024146e-05\n",
            "Loss: 7.8338184e-05\n",
            "Loss: 7.800305e-05\n",
            "Loss: 7.794687e-05\n",
            "Loss: 7.789697e-05\n",
            "Loss: 7.782856e-05\n",
            "Loss: 7.7753335e-05\n",
            "Loss: 7.7708624e-05\n",
            "Loss: 7.764161e-05\n",
            "Loss: 7.760531e-05\n",
            "Loss: 7.7541335e-05\n",
            "Loss: 7.808103e-05\n",
            "Loss: 7.751632e-05\n",
            "Loss: 7.7421784e-05\n",
            "Loss: 7.735034e-05\n",
            "Loss: 7.726193e-05\n",
            "Loss: 7.795521e-05\n",
            "Loss: 7.7235476e-05\n",
            "Loss: 7.719977e-05\n",
            "Loss: 7.712452e-05\n",
            "Loss: 7.70543e-05\n",
            "Loss: 7.701418e-05\n",
            "Loss: 7.697331e-05\n",
            "Loss: 7.694465e-05\n",
            "Loss: 7.690824e-05\n",
            "Loss: 7.681309e-05\n",
            "Loss: 7.6719574e-05\n",
            "Loss: 7.665502e-05\n",
            "Loss: 7.6598175e-05\n",
            "Loss: 7.657009e-05\n",
            "Loss: 7.653807e-05\n",
            "Loss: 7.646621e-05\n",
            "Loss: 7.6615775e-05\n",
            "Loss: 7.6420634e-05\n",
            "Loss: 7.6360026e-05\n",
            "Loss: 7.6295626e-05\n",
            "Loss: 7.626257e-05\n",
            "Loss: 7.6198805e-05\n",
            "Loss: 7.6299504e-05\n",
            "Loss: 7.61735e-05\n",
            "Loss: 7.613057e-05\n",
            "Loss: 7.608992e-05\n",
            "Loss: 7.6064985e-05\n",
            "Loss: 7.603128e-05\n",
            "Loss: 7.5997465e-05\n",
            "Loss: 7.596971e-05\n",
            "Loss: 7.593761e-05\n",
            "Loss: 7.589163e-05\n",
            "Loss: 7.5847114e-05\n",
            "Loss: 7.580906e-05\n",
            "Loss: 7.57771e-05\n",
            "Loss: 7.576076e-05\n",
            "Loss: 7.571205e-05\n",
            "Loss: 7.5728305e-05\n",
            "Loss: 7.56874e-05\n",
            "Loss: 7.566925e-05\n",
            "Loss: 7.564266e-05\n",
            "Loss: 7.5616495e-05\n",
            "Loss: 7.556969e-05\n",
            "Loss: 7.56659e-05\n",
            "Loss: 7.555523e-05\n",
            "Loss: 7.550832e-05\n",
            "Loss: 7.546956e-05\n",
            "Loss: 7.543301e-05\n",
            "Loss: 7.540129e-05\n",
            "Loss: 7.540441e-05\n",
            "Loss: 7.538145e-05\n",
            "Loss: 7.53387e-05\n",
            "Loss: 7.529282e-05\n",
            "Loss: 7.52424e-05\n",
            "Loss: 7.5206495e-05\n",
            "Loss: 7.5161865e-05\n",
            "Loss: 7.512015e-05\n",
            "Loss: 7.505363e-05\n",
            "Loss: 7.5048185e-05\n",
            "Loss: 7.5029006e-05\n",
            "Loss: 7.498425e-05\n",
            "Loss: 7.494651e-05\n",
            "Loss: 7.491234e-05\n",
            "Loss: 7.488057e-05\n",
            "Loss: 7.484469e-05\n",
            "Loss: 7.482482e-05\n",
            "Loss: 7.479868e-05\n",
            "Loss: 7.4773845e-05\n",
            "Loss: 7.473941e-05\n",
            "Loss: 7.469382e-05\n",
            "Loss: 7.470376e-05\n",
            "Loss: 7.465933e-05\n",
            "Loss: 7.462086e-05\n",
            "Loss: 7.45719e-05\n",
            "Loss: 7.4526e-05\n",
            "Loss: 7.442347e-05\n",
            "Loss: 7.453958e-05\n",
            "Loss: 7.43787e-05\n",
            "Loss: 7.428205e-05\n",
            "Loss: 7.4210366e-05\n",
            "Loss: 7.415761e-05\n",
            "Loss: 7.4094925e-05\n",
            "Loss: 7.421806e-05\n",
            "Loss: 7.40635e-05\n",
            "Loss: 7.3985735e-05\n",
            "Loss: 7.3952586e-05\n",
            "Loss: 7.388316e-05\n",
            "Loss: 7.384729e-05\n",
            "Loss: 7.3803305e-05\n",
            "Loss: 7.3742485e-05\n",
            "Loss: 7.364437e-05\n",
            "Loss: 7.3609706e-05\n",
            "Loss: 7.350861e-05\n",
            "Loss: 7.345966e-05\n",
            "Loss: 7.3393894e-05\n",
            "Loss: 7.331509e-05\n",
            "Loss: 7.3353265e-05\n",
            "Loss: 7.327627e-05\n",
            "Loss: 7.324017e-05\n",
            "Loss: 7.3210394e-05\n",
            "Loss: 7.316361e-05\n",
            "Loss: 7.309546e-05\n",
            "Loss: 7.304984e-05\n",
            "Loss: 7.3009665e-05\n",
            "Loss: 7.297875e-05\n",
            "Loss: 7.295646e-05\n",
            "Loss: 7.293012e-05\n",
            "Loss: 7.290215e-05\n",
            "Loss: 7.285333e-05\n",
            "Loss: 7.2822186e-05\n",
            "Loss: 7.279214e-05\n",
            "Loss: 7.275549e-05\n",
            "Loss: 7.2708375e-05\n",
            "Loss: 7.26657e-05\n",
            "Loss: 7.2639305e-05\n",
            "Loss: 7.261309e-05\n",
            "Loss: 7.257297e-05\n",
            "Loss: 7.2515155e-05\n",
            "Loss: 7.246559e-05\n",
            "Loss: 7.241436e-05\n",
            "Loss: 7.2382936e-05\n",
            "Loss: 7.233162e-05\n",
            "Loss: 7.228268e-05\n",
            "Loss: 7.224038e-05\n",
            "Loss: 7.2175564e-05\n",
            "Loss: 7.214541e-05\n",
            "Loss: 7.209073e-05\n",
            "Loss: 7.202306e-05\n",
            "Loss: 7.194305e-05\n",
            "Loss: 7.188824e-05\n",
            "Loss: 7.185048e-05\n",
            "Loss: 7.181503e-05\n",
            "Loss: 7.1757095e-05\n",
            "Loss: 7.1677794e-05\n",
            "Loss: 7.1589166e-05\n",
            "Loss: 7.1495444e-05\n",
            "Loss: 7.1473405e-05\n",
            "Loss: 7.1391914e-05\n",
            "Loss: 7.1352246e-05\n",
            "Loss: 7.129648e-05\n",
            "Loss: 7.126503e-05\n",
            "Loss: 7.121189e-05\n",
            "Loss: 7.113761e-05\n",
            "Loss: 7.108455e-05\n",
            "Loss: 7.0979775e-05\n",
            "Loss: 7.088182e-05\n",
            "Loss: 7.07726e-05\n",
            "Loss: 7.0702015e-05\n",
            "Loss: 7.0570364e-05\n",
            "Loss: 7.071829e-05\n",
            "Loss: 7.054044e-05\n",
            "Loss: 7.049466e-05\n",
            "Loss: 7.042372e-05\n",
            "Loss: 7.033044e-05\n",
            "Loss: 7.022044e-05\n",
            "Loss: 7.067481e-05\n",
            "Loss: 7.0200964e-05\n",
            "Loss: 7.014514e-05\n",
            "Loss: 7.0109316e-05\n",
            "Loss: 7.0073664e-05\n",
            "Loss: 7.0020826e-05\n",
            "Loss: 6.994049e-05\n",
            "Loss: 6.98751e-05\n",
            "Loss: 6.9865826e-05\n",
            "Loss: 6.9818154e-05\n",
            "Loss: 6.980231e-05\n",
            "Loss: 6.977501e-05\n",
            "Loss: 6.9735805e-05\n",
            "Loss: 6.9709415e-05\n",
            "Loss: 6.967275e-05\n",
            "Loss: 6.965158e-05\n",
            "Loss: 6.961137e-05\n",
            "Loss: 6.957783e-05\n",
            "Loss: 6.9527756e-05\n",
            "Loss: 6.9440546e-05\n",
            "Loss: 6.938004e-05\n",
            "Loss: 6.933302e-05\n",
            "Loss: 6.929394e-05\n",
            "Loss: 6.9242386e-05\n",
            "Loss: 6.916082e-05\n",
            "Loss: 6.929031e-05\n",
            "Loss: 6.912436e-05\n",
            "Loss: 6.9070375e-05\n",
            "Loss: 6.901362e-05\n",
            "Loss: 6.897566e-05\n",
            "Loss: 6.890623e-05\n",
            "Loss: 6.892257e-05\n",
            "Loss: 6.8857225e-05\n",
            "Loss: 6.880301e-05\n",
            "Loss: 6.875037e-05\n",
            "Loss: 6.86951e-05\n",
            "Loss: 6.862101e-05\n",
            "Loss: 6.854044e-05\n",
            "Loss: 6.848062e-05\n",
            "Loss: 6.8440495e-05\n",
            "Loss: 6.837415e-05\n",
            "Loss: 6.8341265e-05\n",
            "Loss: 6.8313355e-05\n",
            "Loss: 6.828571e-05\n",
            "Loss: 6.82657e-05\n",
            "Loss: 6.821428e-05\n",
            "Loss: 6.818297e-05\n",
            "Loss: 6.813262e-05\n",
            "Loss: 6.807421e-05\n",
            "Loss: 6.801958e-05\n",
            "Loss: 6.7978224e-05\n",
            "Loss: 6.794236e-05\n",
            "Loss: 6.78953e-05\n",
            "Loss: 6.783163e-05\n",
            "Loss: 6.779557e-05\n",
            "Loss: 6.776214e-05\n",
            "Loss: 6.77401e-05\n",
            "Loss: 6.768377e-05\n",
            "Loss: 6.763174e-05\n",
            "Loss: 6.757227e-05\n",
            "Loss: 6.756955e-05\n",
            "Loss: 6.7538705e-05\n",
            "Loss: 6.750269e-05\n",
            "Loss: 6.749336e-05\n",
            "Loss: 6.7445435e-05\n",
            "Loss: 6.742681e-05\n",
            "Loss: 6.739712e-05\n",
            "Loss: 6.735511e-05\n",
            "Loss: 6.727771e-05\n",
            "Loss: 6.7392466e-05\n",
            "Loss: 6.725044e-05\n",
            "Loss: 6.721094e-05\n",
            "Loss: 6.717292e-05\n",
            "Loss: 6.712404e-05\n",
            "Loss: 6.7072484e-05\n",
            "Loss: 6.701887e-05\n",
            "Loss: 6.6990535e-05\n",
            "Loss: 6.694921e-05\n",
            "Loss: 6.688399e-05\n",
            "Loss: 6.684154e-05\n",
            "Loss: 6.6783476e-05\n",
            "Loss: 6.675153e-05\n",
            "Loss: 6.671652e-05\n",
            "Loss: 6.669114e-05\n",
            "Loss: 6.6646986e-05\n",
            "Loss: 6.660845e-05\n",
            "Loss: 6.655604e-05\n",
            "Loss: 6.649633e-05\n",
            "Loss: 6.6472196e-05\n",
            "Loss: 6.63962e-05\n",
            "Loss: 6.6368884e-05\n",
            "Loss: 6.634228e-05\n",
            "Loss: 6.630754e-05\n",
            "Loss: 6.62645e-05\n",
            "Loss: 6.6224464e-05\n",
            "Loss: 6.616737e-05\n",
            "Loss: 6.6205226e-05\n",
            "Loss: 6.6147724e-05\n",
            "Loss: 6.6121254e-05\n",
            "Loss: 6.604865e-05\n",
            "Loss: 6.600814e-05\n",
            "Loss: 6.595081e-05\n",
            "Loss: 6.587698e-05\n",
            "Loss: 6.582143e-05\n",
            "Loss: 6.5778324e-05\n",
            "Loss: 6.574197e-05\n",
            "Loss: 6.5717104e-05\n",
            "Loss: 6.564753e-05\n",
            "Loss: 6.560818e-05\n",
            "Loss: 6.56638e-05\n",
            "Loss: 6.558922e-05\n",
            "Loss: 6.5563305e-05\n",
            "Loss: 6.551867e-05\n",
            "Loss: 6.545966e-05\n",
            "Loss: 6.537542e-05\n",
            "Loss: 6.52903e-05\n",
            "Loss: 6.522634e-05\n",
            "Loss: 6.517956e-05\n",
            "Loss: 6.514524e-05\n",
            "Loss: 6.5117754e-05\n",
            "Loss: 6.5118154e-05\n",
            "Loss: 6.508807e-05\n",
            "Loss: 6.505257e-05\n",
            "Loss: 6.500207e-05\n",
            "Loss: 6.497555e-05\n",
            "Loss: 6.491991e-05\n",
            "Loss: 6.486936e-05\n",
            "Loss: 6.481143e-05\n",
            "Loss: 6.477578e-05\n",
            "Loss: 6.474846e-05\n",
            "Loss: 6.471014e-05\n",
            "Loss: 6.4674605e-05\n",
            "Loss: 6.462113e-05\n",
            "Loss: 6.4567015e-05\n",
            "Loss: 6.4470005e-05\n",
            "Loss: 6.444207e-05\n",
            "Loss: 6.437978e-05\n",
            "Loss: 6.435266e-05\n",
            "Loss: 6.4286985e-05\n",
            "Loss: 6.424743e-05\n",
            "Loss: 6.4197666e-05\n",
            "Loss: 6.41421e-05\n",
            "Loss: 6.409433e-05\n",
            "Loss: 6.400385e-05\n",
            "Loss: 6.427725e-05\n",
            "Loss: 6.397816e-05\n",
            "Loss: 6.3916916e-05\n",
            "Loss: 6.3871616e-05\n",
            "Loss: 6.384117e-05\n",
            "Loss: 6.380169e-05\n",
            "Loss: 6.377266e-05\n",
            "Loss: 6.373277e-05\n",
            "Loss: 6.372385e-05\n",
            "Loss: 6.368454e-05\n",
            "Loss: 6.365979e-05\n",
            "Loss: 6.3614156e-05\n",
            "Loss: 6.3569205e-05\n",
            "Loss: 6.356038e-05\n",
            "Loss: 6.348657e-05\n",
            "Loss: 6.346152e-05\n",
            "Loss: 6.3433996e-05\n",
            "Loss: 6.338956e-05\n",
            "Loss: 6.3345506e-05\n",
            "Loss: 6.3281856e-05\n",
            "Loss: 6.3288295e-05\n",
            "Loss: 6.3258994e-05\n",
            "Loss: 6.3212035e-05\n",
            "Loss: 6.316675e-05\n",
            "Loss: 6.31119e-05\n",
            "Loss: 6.306573e-05\n",
            "Loss: 6.302158e-05\n",
            "Loss: 6.299101e-05\n",
            "Loss: 6.2954314e-05\n",
            "Loss: 6.290424e-05\n",
            "Loss: 6.286125e-05\n",
            "Loss: 6.321051e-05\n",
            "Loss: 6.2846375e-05\n",
            "Loss: 6.279508e-05\n",
            "Loss: 6.275225e-05\n",
            "Loss: 6.2700594e-05\n",
            "Loss: 6.2654144e-05\n",
            "Loss: 6.2654864e-05\n",
            "Loss: 6.262223e-05\n",
            "Loss: 6.257078e-05\n",
            "Loss: 6.252248e-05\n",
            "Loss: 6.2425075e-05\n",
            "Loss: 6.255566e-05\n",
            "Loss: 6.239851e-05\n",
            "Loss: 6.232517e-05\n",
            "Loss: 6.2268555e-05\n",
            "Loss: 6.220411e-05\n",
            "Loss: 6.214558e-05\n",
            "Loss: 6.209514e-05\n",
            "Loss: 6.202913e-05\n",
            "Loss: 6.1981584e-05\n",
            "Loss: 6.194544e-05\n",
            "Loss: 6.1900995e-05\n",
            "Loss: 6.183804e-05\n",
            "Loss: 6.185644e-05\n",
            "Loss: 6.180604e-05\n",
            "Loss: 6.176889e-05\n",
            "Loss: 6.1723644e-05\n",
            "Loss: 6.16951e-05\n",
            "Loss: 6.162158e-05\n",
            "Loss: 6.1533574e-05\n",
            "Loss: 6.143748e-05\n",
            "Loss: 6.138279e-05\n",
            "Loss: 6.134437e-05\n",
            "Loss: 6.131519e-05\n",
            "Loss: 6.12469e-05\n",
            "Loss: 6.119444e-05\n",
            "Loss: 6.11425e-05\n",
            "Loss: 6.108222e-05\n",
            "Loss: 6.103589e-05\n",
            "Loss: 6.0991417e-05\n",
            "Loss: 6.0948107e-05\n",
            "Loss: 6.0975704e-05\n",
            "Loss: 6.0924278e-05\n",
            "Loss: 6.0893428e-05\n",
            "Loss: 6.0860224e-05\n",
            "Loss: 6.0824917e-05\n",
            "Loss: 6.075572e-05\n",
            "Loss: 6.076952e-05\n",
            "Loss: 6.0716156e-05\n",
            "Loss: 6.0634935e-05\n",
            "Loss: 6.0573308e-05\n",
            "Loss: 6.0514154e-05\n",
            "Loss: 6.0456736e-05\n",
            "Loss: 6.0553175e-05\n",
            "Loss: 6.0417344e-05\n",
            "Loss: 6.0377137e-05\n",
            "Loss: 6.03402e-05\n",
            "Loss: 6.0312756e-05\n",
            "Loss: 6.026715e-05\n",
            "Loss: 6.015208e-05\n",
            "Loss: 6.0117236e-05\n",
            "Loss: 6.00677e-05\n",
            "Loss: 6.0043974e-05\n",
            "Loss: 6.002491e-05\n",
            "Loss: 5.9987902e-05\n",
            "Loss: 5.9939466e-05\n",
            "Loss: 5.9918064e-05\n",
            "Loss: 5.987987e-05\n",
            "Loss: 5.985736e-05\n",
            "Loss: 5.9829697e-05\n",
            "Loss: 5.9784325e-05\n",
            "Loss: 5.9719736e-05\n",
            "Loss: 5.9672362e-05\n",
            "Loss: 5.9627004e-05\n",
            "Loss: 5.9599108e-05\n",
            "Loss: 5.954807e-05\n",
            "Loss: 5.9529237e-05\n",
            "Loss: 5.9441933e-05\n",
            "Loss: 5.9414888e-05\n",
            "Loss: 5.9354665e-05\n",
            "Loss: 5.926459e-05\n",
            "Loss: 5.93071e-05\n",
            "Loss: 5.9225815e-05\n",
            "Loss: 5.9157424e-05\n",
            "Loss: 5.9113958e-05\n",
            "Loss: 5.9064452e-05\n",
            "Loss: 5.905282e-05\n",
            "Loss: 5.8987705e-05\n",
            "Loss: 5.896081e-05\n",
            "Loss: 5.8926445e-05\n",
            "Loss: 5.8895792e-05\n",
            "Loss: 5.88573e-05\n",
            "Loss: 5.8743077e-05\n",
            "Loss: 5.8763413e-05\n",
            "Loss: 5.8708352e-05\n",
            "Loss: 5.8661346e-05\n",
            "Loss: 5.8620462e-05\n",
            "Loss: 5.8567945e-05\n",
            "Loss: 5.8519952e-05\n",
            "Loss: 5.8475605e-05\n",
            "Loss: 5.8444326e-05\n",
            "Loss: 5.841246e-05\n",
            "Loss: 5.837716e-05\n",
            "Loss: 5.8330144e-05\n",
            "Loss: 5.8286812e-05\n",
            "Loss: 5.826269e-05\n",
            "Loss: 5.8238944e-05\n",
            "Loss: 5.8208367e-05\n",
            "Loss: 5.815114e-05\n",
            "Loss: 5.807734e-05\n",
            "Loss: 5.8148482e-05\n",
            "Loss: 5.804331e-05\n",
            "Loss: 5.799849e-05\n",
            "Loss: 5.7971592e-05\n",
            "Loss: 5.794391e-05\n",
            "Loss: 5.7909812e-05\n",
            "Loss: 5.787566e-05\n",
            "Loss: 5.7848585e-05\n",
            "Loss: 5.7818077e-05\n",
            "Loss: 5.7778198e-05\n",
            "Loss: 5.7700174e-05\n",
            "Loss: 5.767032e-05\n",
            "Loss: 5.7622026e-05\n",
            "Loss: 5.7597474e-05\n",
            "Loss: 5.754515e-05\n",
            "Loss: 5.7502628e-05\n",
            "Loss: 5.744469e-05\n",
            "Loss: 5.7400553e-05\n",
            "Loss: 5.736808e-05\n",
            "Loss: 5.73161e-05\n",
            "Loss: 5.727195e-05\n",
            "Loss: 5.723221e-05\n",
            "Loss: 5.719858e-05\n",
            "Loss: 5.715742e-05\n",
            "Loss: 5.7121622e-05\n",
            "Loss: 5.7092788e-05\n",
            "Loss: 5.7066736e-05\n",
            "Loss: 5.7049398e-05\n",
            "Loss: 5.6998342e-05\n",
            "Loss: 5.704671e-05\n",
            "Loss: 5.697536e-05\n",
            "Loss: 5.692372e-05\n",
            "Loss: 5.6875346e-05\n",
            "Loss: 5.6839923e-05\n",
            "Loss: 5.680798e-05\n",
            "Loss: 5.677091e-05\n",
            "Loss: 5.6722714e-05\n",
            "Loss: 5.6651545e-05\n",
            "Loss: 5.6582576e-05\n",
            "Loss: 5.6536755e-05\n",
            "Loss: 5.6464192e-05\n",
            "Loss: 5.6437275e-05\n",
            "Loss: 5.641269e-05\n",
            "Loss: 5.639218e-05\n",
            "Loss: 5.635203e-05\n",
            "Loss: 5.6282246e-05\n",
            "Loss: 5.62484e-05\n",
            "Loss: 5.6192937e-05\n",
            "Loss: 5.616859e-05\n",
            "Loss: 5.6126766e-05\n",
            "Loss: 5.6091518e-05\n",
            "Loss: 5.603229e-05\n",
            "Loss: 5.596094e-05\n",
            "Loss: 5.5949346e-05\n",
            "Loss: 5.585663e-05\n",
            "Loss: 5.5821936e-05\n",
            "Loss: 5.5798126e-05\n",
            "Loss: 5.5762477e-05\n",
            "Loss: 5.578782e-05\n",
            "Loss: 5.5743592e-05\n",
            "Loss: 5.5701603e-05\n",
            "Loss: 5.5665565e-05\n",
            "Loss: 5.5611767e-05\n",
            "Loss: 5.5563047e-05\n",
            "Loss: 5.553529e-05\n",
            "Loss: 5.550555e-05\n",
            "Loss: 5.548865e-05\n",
            "Loss: 5.546036e-05\n",
            "Loss: 5.5420467e-05\n",
            "Loss: 5.5359385e-05\n",
            "Loss: 5.5314405e-05\n",
            "Loss: 5.52817e-05\n",
            "Loss: 5.525031e-05\n",
            "Loss: 5.5228134e-05\n",
            "Loss: 5.5192668e-05\n",
            "Loss: 5.5138917e-05\n",
            "Loss: 5.51158e-05\n",
            "Loss: 5.5077307e-05\n",
            "Loss: 5.5036635e-05\n",
            "Loss: 5.496836e-05\n",
            "Loss: 5.5010292e-05\n",
            "Loss: 5.4940792e-05\n",
            "Loss: 5.4894364e-05\n",
            "Loss: 5.486061e-05\n",
            "Loss: 5.4830714e-05\n",
            "Loss: 5.4794695e-05\n",
            "Loss: 5.4756405e-05\n",
            "Loss: 5.4718992e-05\n",
            "Loss: 5.472431e-05\n",
            "Loss: 5.4699714e-05\n",
            "Loss: 5.467992e-05\n",
            "Loss: 5.464939e-05\n",
            "Loss: 5.462233e-05\n",
            "Loss: 5.4576038e-05\n",
            "Loss: 5.4677716e-05\n",
            "Loss: 5.4549877e-05\n",
            "Loss: 5.4515895e-05\n",
            "Loss: 5.448282e-05\n",
            "Loss: 5.4476957e-05\n",
            "Loss: 5.445168e-05\n",
            "Loss: 5.4434575e-05\n",
            "Loss: 5.441779e-05\n",
            "Loss: 5.4401942e-05\n",
            "Loss: 5.444252e-05\n",
            "Loss: 5.4385688e-05\n",
            "Loss: 5.435e-05\n",
            "Loss: 5.4300515e-05\n",
            "Loss: 5.4254215e-05\n",
            "Loss: 5.4212665e-05\n",
            "Loss: 5.4146058e-05\n",
            "Loss: 5.4072516e-05\n",
            "Loss: 5.403636e-05\n",
            "Loss: 5.3987264e-05\n",
            "Loss: 5.396144e-05\n",
            "Loss: 5.392259e-05\n",
            "Loss: 5.396881e-05\n",
            "Loss: 5.390124e-05\n",
            "Loss: 5.3853495e-05\n",
            "Loss: 5.382032e-05\n",
            "Loss: 5.3780394e-05\n",
            "Loss: 5.3735144e-05\n",
            "Loss: 5.367228e-05\n",
            "Loss: 5.4717464e-05\n",
            "Loss: 5.365038e-05\n",
            "Loss: 5.3577693e-05\n",
            "Loss: 5.352478e-05\n",
            "Loss: 5.342783e-05\n",
            "Loss: 5.336981e-05\n",
            "Loss: 5.3315674e-05\n",
            "Loss: 5.3279262e-05\n",
            "Loss: 5.324672e-05\n",
            "Loss: 5.3224518e-05\n",
            "Loss: 5.3179676e-05\n",
            "Loss: 5.3118685e-05\n",
            "Loss: 5.3082134e-05\n",
            "Loss: 5.3024523e-05\n",
            "Loss: 5.3005133e-05\n",
            "Loss: 5.2964464e-05\n",
            "Loss: 5.293335e-05\n",
            "Loss: 5.2893272e-05\n",
            "Loss: 5.2861848e-05\n",
            "Loss: 5.2831536e-05\n",
            "Loss: 5.280519e-05\n",
            "Loss: 5.275815e-05\n",
            "Loss: 5.2693147e-05\n",
            "Loss: 5.261546e-05\n",
            "Loss: 5.2574345e-05\n",
            "Loss: 5.2542462e-05\n",
            "Loss: 5.2488118e-05\n",
            "Loss: 5.2415715e-05\n",
            "Loss: 5.2396703e-05\n",
            "Loss: 5.2320214e-05\n",
            "Loss: 5.2291856e-05\n",
            "Loss: 5.2257477e-05\n",
            "Loss: 5.2220297e-05\n",
            "Loss: 5.278356e-05\n",
            "Loss: 5.2205825e-05\n",
            "Loss: 5.2165342e-05\n",
            "Loss: 5.2129682e-05\n",
            "Loss: 5.2097996e-05\n",
            "Loss: 5.2071373e-05\n",
            "Loss: 5.2047748e-05\n",
            "Loss: 5.2013056e-05\n",
            "Loss: 5.199633e-05\n",
            "Loss: 5.1958785e-05\n",
            "Loss: 5.19283e-05\n",
            "Loss: 5.1882314e-05\n",
            "Loss: 5.1838844e-05\n",
            "Loss: 5.1732088e-05\n",
            "Loss: 5.167898e-05\n",
            "Loss: 5.163947e-05\n",
            "Loss: 5.1599513e-05\n",
            "Loss: 5.1550494e-05\n",
            "Loss: 5.146839e-05\n",
            "Loss: 5.134559e-05\n",
            "Loss: 5.1292198e-05\n",
            "Loss: 5.121383e-05\n",
            "Loss: 5.1175033e-05\n",
            "Loss: 5.112784e-05\n",
            "Loss: 5.106398e-05\n",
            "Loss: 5.096619e-05\n",
            "Loss: 5.1185845e-05\n",
            "Loss: 5.0923896e-05\n",
            "Loss: 5.0852424e-05\n",
            "Loss: 5.080022e-05\n",
            "Loss: 5.074186e-05\n",
            "Loss: 5.0664232e-05\n",
            "Loss: 5.0580504e-05\n",
            "Loss: 5.052887e-05\n",
            "Loss: 5.0454943e-05\n",
            "Loss: 5.036906e-05\n",
            "Loss: 5.0303082e-05\n",
            "Loss: 5.0249215e-05\n",
            "Loss: 5.0193812e-05\n",
            "Loss: 5.0140625e-05\n",
            "Loss: 5.0010316e-05\n",
            "Loss: 4.9945185e-05\n",
            "Loss: 4.989058e-05\n",
            "Loss: 4.9850256e-05\n",
            "Loss: 4.9812505e-05\n",
            "Loss: 4.9786624e-05\n",
            "Loss: 4.973563e-05\n",
            "Loss: 4.9664966e-05\n",
            "Loss: 4.9634473e-05\n",
            "Loss: 4.9529423e-05\n",
            "Loss: 4.9485498e-05\n",
            "Loss: 4.945215e-05\n",
            "Loss: 4.9410733e-05\n",
            "Loss: 4.936992e-05\n",
            "Loss: 4.9331607e-05\n",
            "Loss: 4.928832e-05\n",
            "Loss: 5.0060575e-05\n",
            "Loss: 4.927956e-05\n",
            "Loss: 4.923532e-05\n",
            "Loss: 4.9203514e-05\n",
            "Loss: 4.920886e-05\n",
            "Loss: 4.9175665e-05\n",
            "Loss: 4.9130635e-05\n",
            "Loss: 4.909291e-05\n",
            "Loss: 4.9037393e-05\n",
            "Loss: 4.898174e-05\n",
            "Loss: 4.9012324e-05\n",
            "Loss: 4.8944297e-05\n",
            "Loss: 4.8889975e-05\n",
            "Loss: 4.8826492e-05\n",
            "Loss: 4.8756792e-05\n",
            "Loss: 4.8729467e-05\n",
            "Loss: 4.8685957e-05\n",
            "Loss: 4.8667196e-05\n",
            "Loss: 4.864343e-05\n",
            "Loss: 4.860103e-05\n",
            "Loss: 4.8546703e-05\n",
            "Loss: 4.850915e-05\n",
            "Loss: 4.847413e-05\n",
            "Loss: 4.8458125e-05\n",
            "Loss: 4.843692e-05\n",
            "Loss: 4.838668e-05\n",
            "Loss: 4.8319253e-05\n",
            "Loss: 4.8449365e-05\n",
            "Loss: 4.8283357e-05\n",
            "Loss: 4.8223545e-05\n",
            "Loss: 4.8186394e-05\n",
            "Loss: 4.8149384e-05\n",
            "Loss: 4.8095637e-05\n",
            "Loss: 4.8038208e-05\n",
            "Loss: 4.799094e-05\n",
            "Loss: 4.7957357e-05\n",
            "Loss: 4.7899422e-05\n",
            "Loss: 4.7858433e-05\n",
            "Loss: 4.780724e-05\n",
            "Loss: 4.7779227e-05\n",
            "Loss: 4.775077e-05\n",
            "Loss: 4.7719033e-05\n",
            "Loss: 4.7663074e-05\n",
            "Loss: 4.7686888e-05\n",
            "Loss: 4.763977e-05\n",
            "Loss: 4.7614933e-05\n",
            "Loss: 4.75977e-05\n",
            "Loss: 4.7580244e-05\n",
            "Loss: 4.7545538e-05\n",
            "Loss: 4.7501395e-05\n",
            "Loss: 4.7468304e-05\n",
            "Loss: 4.7425434e-05\n",
            "Loss: 4.7378504e-05\n",
            "Loss: 4.734336e-05\n",
            "Loss: 4.7285812e-05\n",
            "Loss: 4.723997e-05\n",
            "Loss: 4.7181624e-05\n",
            "Loss: 4.715109e-05\n",
            "Loss: 4.7101697e-05\n",
            "Loss: 4.7081507e-05\n",
            "Loss: 4.7054833e-05\n",
            "Loss: 4.7028574e-05\n",
            "Loss: 4.6983187e-05\n",
            "Loss: 4.6944882e-05\n",
            "Loss: 4.691624e-05\n",
            "Loss: 4.688934e-05\n",
            "Loss: 4.686814e-05\n",
            "Loss: 4.681756e-05\n",
            "Loss: 4.6747304e-05\n",
            "Loss: 4.6833535e-05\n",
            "Loss: 4.6711582e-05\n",
            "Loss: 4.66657e-05\n",
            "Loss: 4.66441e-05\n",
            "Loss: 4.66176e-05\n",
            "Loss: 4.658194e-05\n",
            "Loss: 4.6516856e-05\n",
            "Loss: 4.648773e-05\n",
            "Loss: 4.645945e-05\n",
            "Loss: 4.6442205e-05\n",
            "Loss: 4.6417146e-05\n",
            "Loss: 4.6378766e-05\n",
            "Loss: 4.6328598e-05\n",
            "Loss: 4.640487e-05\n",
            "Loss: 4.6306865e-05\n",
            "Loss: 4.6257777e-05\n",
            "Loss: 4.62139e-05\n",
            "Loss: 4.6139718e-05\n",
            "Loss: 4.607939e-05\n",
            "Loss: 4.6049696e-05\n",
            "Loss: 4.6021807e-05\n",
            "Loss: 4.5992358e-05\n",
            "Loss: 4.5969595e-05\n",
            "Loss: 4.5923953e-05\n",
            "Loss: 4.5912755e-05\n",
            "Loss: 4.5848283e-05\n",
            "Loss: 4.5810808e-05\n",
            "Loss: 4.5765046e-05\n",
            "Loss: 4.5720608e-05\n",
            "Loss: 4.562254e-05\n",
            "Loss: 4.5670116e-05\n",
            "Loss: 4.5568817e-05\n",
            "Loss: 4.5511864e-05\n",
            "Loss: 4.5480905e-05\n",
            "Loss: 4.546222e-05\n",
            "Loss: 4.54119e-05\n",
            "Loss: 4.5346933e-05\n",
            "Loss: 4.5247558e-05\n",
            "Loss: 4.5275916e-05\n",
            "Loss: 4.5207e-05\n",
            "Loss: 4.5157467e-05\n",
            "Loss: 4.5136178e-05\n",
            "Loss: 4.51096e-05\n",
            "Loss: 4.5078963e-05\n",
            "Loss: 4.503088e-05\n",
            "Loss: 4.5013872e-05\n",
            "Loss: 4.499037e-05\n",
            "Loss: 4.496191e-05\n",
            "Loss: 4.4937573e-05\n",
            "Loss: 4.488408e-05\n",
            "Loss: 4.485837e-05\n",
            "Loss: 4.4815388e-05\n",
            "Loss: 4.4794473e-05\n",
            "Loss: 4.475729e-05\n",
            "Loss: 4.4696673e-05\n",
            "Loss: 4.479809e-05\n",
            "Loss: 4.4671375e-05\n",
            "Loss: 4.462075e-05\n",
            "Loss: 4.4596858e-05\n",
            "Loss: 4.4581502e-05\n",
            "Loss: 4.454345e-05\n",
            "Loss: 4.4516273e-05\n",
            "Loss: 4.4478318e-05\n",
            "Loss: 4.4441564e-05\n",
            "Loss: 4.443254e-05\n",
            "Loss: 4.438711e-05\n",
            "Loss: 4.437163e-05\n",
            "Loss: 4.434355e-05\n",
            "Loss: 4.4296416e-05\n",
            "Loss: 4.446344e-05\n",
            "Loss: 4.4278935e-05\n",
            "Loss: 4.4239303e-05\n",
            "Loss: 4.42112e-05\n",
            "Loss: 4.4160464e-05\n",
            "Loss: 4.4101645e-05\n",
            "Loss: 4.4453478e-05\n",
            "Loss: 4.4083114e-05\n",
            "Loss: 4.403553e-05\n",
            "Loss: 4.400235e-05\n",
            "Loss: 4.39339e-05\n",
            "Loss: 4.3914355e-05\n",
            "Loss: 4.3850057e-05\n",
            "Loss: 4.382464e-05\n",
            "Loss: 4.380045e-05\n",
            "Loss: 4.379687e-05\n",
            "Loss: 4.3781278e-05\n",
            "Loss: 4.375856e-05\n",
            "Loss: 4.3707412e-05\n",
            "Loss: 4.367764e-05\n",
            "Loss: 4.3647546e-05\n",
            "Loss: 4.363017e-05\n",
            "Loss: 4.3600507e-05\n",
            "Loss: 4.356647e-05\n",
            "Loss: 4.3517204e-05\n",
            "Loss: 4.3509044e-05\n",
            "Loss: 4.343978e-05\n",
            "Loss: 4.3415188e-05\n",
            "Loss: 4.3377335e-05\n",
            "Loss: 4.334632e-05\n",
            "Loss: 4.3321765e-05\n",
            "Loss: 4.3301556e-05\n",
            "Loss: 4.328129e-05\n",
            "Loss: 4.3264106e-05\n",
            "Loss: 4.3218068e-05\n",
            "Loss: 4.315721e-05\n",
            "Loss: 4.3284577e-05\n",
            "Loss: 4.313099e-05\n",
            "Loss: 4.309184e-05\n",
            "Loss: 4.3065906e-05\n",
            "Loss: 4.3046544e-05\n",
            "Loss: 4.3016953e-05\n",
            "Loss: 4.2991327e-05\n",
            "Loss: 4.2969943e-05\n",
            "Loss: 4.294492e-05\n",
            "Loss: 4.290075e-05\n",
            "Loss: 4.28837e-05\n",
            "Loss: 4.2849006e-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-daeccee979c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m#model.train(50000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-daeccee979c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nIter)\u001b[0m\n\u001b[1;32m    188\u001b[0m                                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                                 loss_callback = self.callback)        \n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, session, feed_dict, fetches, step_callback, loss_callback, **run_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mpacked_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mstep_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         optimizer_kwargs=self.optimizer_kwargs)\n\u001b[0m\u001b[1;32m    208\u001b[0m     var_vals = [\n\u001b[1;32m    209\u001b[0m         \u001b[0mpacked_var_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpacking_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpacking_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packing_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36m_minimize\u001b[0;34m(self, initial_val, loss_grad_func, equality_funcs, equality_grad_funcs, inequality_funcs, inequality_grad_funcs, packed_bounds, step_callback, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mminimize_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mminimize_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     message_lines = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 624\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    625\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36mloss_grad_func_wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_grad_func_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0;31m# SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_grad_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36meval_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       augmented_fetch_vals = session.run(\n\u001b[0;32m--> 278\u001b[0;31m           augmented_fetches, feed_dict=augmented_feed_dict)\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1wxB-1evyBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}